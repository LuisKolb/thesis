{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.retrieval.retrieve import get_retriever, retrieve_evidence, AuredDataset\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, root_dir\n",
    "\n",
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "# walk through the pkl directory and load all the datasets in one of its subdirectories\n",
    "# load each dataset with its subdirectory name and filename as the key\n",
    "# skip non-pkl files\n",
    "for subdir in os.listdir(pkl_dir):\n",
    "    if not os.path.isdir(os.path.join(pkl_dir, subdir)):\n",
    "        continue            \n",
    "    datasets[subdir] = {}\n",
    "    for filename in os.listdir(os.path.join(pkl_dir, subdir)):\n",
    "        if not filename.endswith('.pkl'):\n",
    "            continue\n",
    "        key = os.path.join(subdir, filename)\n",
    "        datasets[subdir][filename.split('.')[0]] = load_pkl(os.path.join(pkl_dir, key))\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "# ground truth RQ1\n",
    "golden = ptio.read_qrels(os.path.join(root_dir, 'data', qrel_filename))\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "# these selected variations are selected for these reasons:\n",
    "# - pre-nonam-nobio     (\"raw\" data, but preprocessed)\n",
    "# - pre-nam-bio         (we would expect lexical retrieval to be best here)\n",
    "# - nopre-nonam-nobio   (\"raw\" data)\n",
    "# - nopre-nam-bio       (we would expect semantic retrieval to be best here, most information contained here)\n",
    "selected_variations = [\"pre-nonam-nobio\", \"pre-nam-bio\", \"nopre-nonam-nobio\", \"nopre-nam-bio\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HFSentenceTransformersRetriever with model: sentence-transformers/multi-qa-distilbert-cos-v1\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "Initializing CrossEncoderRetriever with model: cross-encoder/stsb-roberta-large\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bm25': <lkae.retrieval.methods.bm25.BM25Retriever at 0x191d6221510>,\n",
       " 'openai': <lkae.retrieval.methods.openai_embeddings.OpenAIRetriever at 0x191d6223a90>,\n",
       " 'sent-transformers-hf': <lkae.retrieval.methods.sent_transformers_hf.HFSentenceTransformersRetriever at 0x1921874b370>,\n",
       " 'sent-transformers-local': <lkae.retrieval.methods.sent_transformers.SBERTRetriever at 0x19218748d90>,\n",
       " 'crossencoder': <lkae.retrieval.methods.sent_transformers.CrossEncoderRetriever at 0x1922481e410>,\n",
       " 'tfidf': <lkae.retrieval.methods.tfidf.TFIDFRetriever at 0x19225debf40>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "\n",
    "retrievers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        retriever_label = get_retriever(**config)\n",
    "        retrievers[config['retriever_method']] = retriever_label\n",
    "\n",
    "retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for retrieval run - R@5: 0.6345 MAP: 0.5679 with config\tretriever: bm25;\tds: pre-nonam-nobio, took 2.07 seconds\n",
      "result for retrieval run - R@5: 0.6186 MAP: 0.5719 with config\tretriever: openai;\tds: pre-nonam-nobio, took 143.42 seconds\n",
      "result for retrieval run - R@5: 0.5972 MAP: 0.5525 with config\tretriever: sent-transformers-hf;\tds: pre-nonam-nobio, took 303.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisk\\miniconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for retrieval run - R@5: 0.5968 MAP: 0.5550 with config\tretriever: sent-transformers-local;\tds: pre-nonam-nobio, took 20.21 seconds\n",
      "result for retrieval run - R@5: 0.6433 MAP: 0.5691 with config\tretriever: crossencoder;\tds: pre-nonam-nobio, took 681.92 seconds\n",
      "result for retrieval run - R@5: 0.6022 MAP: 0.5045 with config\tretriever: tfidf;\tds: pre-nonam-nobio, took 0.97 seconds\n",
      "result for retrieval run - R@5: 0.6457 MAP: 0.5809 with config\tretriever: bm25;\tds: pre-nam-bio, took 2.88 seconds\n",
      "result for retrieval run - R@5: 0.6339 MAP: 0.5756 with config\tretriever: openai;\tds: pre-nam-bio, took 183.76 seconds\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for retrieval run - R@5: 0.5542 MAP: 0.5076 with config\tretriever: sent-transformers-hf;\tds: pre-nam-bio, took 481.84 seconds\n",
      "result for retrieval run - R@5: 0.5681 MAP: 0.5204 with config\tretriever: sent-transformers-local;\tds: pre-nam-bio, took 25.70 seconds\n",
      "result for retrieval run - R@5: 0.6025 MAP: 0.5325 with config\tretriever: crossencoder;\tds: pre-nam-bio, took 926.97 seconds\n",
      "result for retrieval run - R@5: 0.5984 MAP: 0.5382 with config\tretriever: tfidf;\tds: pre-nam-bio, took 1.22 seconds\n",
      "result for retrieval run - R@5: 0.6207 MAP: 0.5588 with config\tretriever: bm25;\tds: nopre-nonam-nobio, took 2.26 seconds\n",
      "result for retrieval run - R@5: 0.6318 MAP: 0.5888 with config\tretriever: openai;\tds: nopre-nonam-nobio, took 182.61 seconds\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for retrieval run - R@5: 0.5917 MAP: 0.5481 with config\tretriever: sent-transformers-hf;\tds: nopre-nonam-nobio, took 394.12 seconds\n",
      "result for retrieval run - R@5: 0.5965 MAP: 0.5638 with config\tretriever: sent-transformers-local;\tds: nopre-nonam-nobio, took 25.43 seconds\n",
      "result for retrieval run - R@5: 0.5773 MAP: 0.5388 with config\tretriever: crossencoder;\tds: nopre-nonam-nobio, took 1280.25 seconds\n",
      "result for retrieval run - R@5: 0.6117 MAP: 0.4918 with config\tretriever: tfidf;\tds: nopre-nonam-nobio, took 1.06 seconds\n",
      "result for retrieval run - R@5: 0.6378 MAP: 0.5651 with config\tretriever: bm25;\tds: nopre-nam-bio, took 3.13 seconds\n",
      "result for retrieval run - R@5: 0.6288 MAP: 0.5683 with config\tretriever: openai;\tds: nopre-nam-bio, took 165.78 seconds\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for retrieval run - R@5: 0.5886 MAP: 0.5601 with config\tretriever: sent-transformers-hf;\tds: nopre-nam-bio, took 554.41 seconds\n",
      "result for retrieval run - R@5: 0.5844 MAP: 0.5347 with config\tretriever: sent-transformers-local;\tds: nopre-nam-bio, took 35.19 seconds\n",
      "result for retrieval run - R@5: 0.5298 MAP: 0.4662 with config\tretriever: crossencoder;\tds: nopre-nam-bio, took 2290.55 seconds\n",
      "result for retrieval run - R@5: 0.6190 MAP: 0.5319 with config\tretriever: tfidf;\tds: nopre-nam-bio, took 1.36 seconds\n",
      "saved df to results/df_retrieval.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R5</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Retrieval_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.645675</td>\n",
       "      <td>0.580895</td>\n",
       "      <td>bm25</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>2.881830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.643261</td>\n",
       "      <td>0.569146</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>681.923324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.637772</td>\n",
       "      <td>0.565110</td>\n",
       "      <td>bm25</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>3.133084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.634475</td>\n",
       "      <td>0.567864</td>\n",
       "      <td>bm25</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>2.066375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.633950</td>\n",
       "      <td>0.575602</td>\n",
       "      <td>openai</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>183.755365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.631840</td>\n",
       "      <td>0.588819</td>\n",
       "      <td>openai</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>182.610973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.628753</td>\n",
       "      <td>0.568285</td>\n",
       "      <td>openai</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>165.777771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.620712</td>\n",
       "      <td>0.558793</td>\n",
       "      <td>bm25</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>2.256596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.618952</td>\n",
       "      <td>0.531911</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>1.360650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.571916</td>\n",
       "      <td>openai</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>143.422597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.611667</td>\n",
       "      <td>0.491801</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>1.063158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.602466</td>\n",
       "      <td>0.532471</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>926.972076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.602172</td>\n",
       "      <td>0.504463</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.970865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.598374</td>\n",
       "      <td>0.538183</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>1.217622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.597218</td>\n",
       "      <td>0.552529</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>303.729978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.596751</td>\n",
       "      <td>0.554987</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>20.212909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.596546</td>\n",
       "      <td>0.563793</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>25.425695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.591654</td>\n",
       "      <td>0.548149</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>394.124445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.588561</td>\n",
       "      <td>0.560076</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>554.409397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.584392</td>\n",
       "      <td>0.534687</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>35.194071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.577275</td>\n",
       "      <td>0.538750</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>1280.249461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.568142</td>\n",
       "      <td>0.520372</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>25.702166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.554250</td>\n",
       "      <td>0.507555</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>481.835307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.529836</td>\n",
       "      <td>0.466154</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>2290.554706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          R5       MAP         Retrieval_Method        DS_Settings  \\\n",
       "6   0.645675  0.580895                     bm25        pre-nam-bio   \n",
       "4   0.643261  0.569146             crossencoder    pre-nonam-nobio   \n",
       "18  0.637772  0.565110                     bm25      nopre-nam-bio   \n",
       "0   0.634475  0.567864                     bm25    pre-nonam-nobio   \n",
       "7   0.633950  0.575602                   openai        pre-nam-bio   \n",
       "13  0.631840  0.588819                   openai  nopre-nonam-nobio   \n",
       "19  0.628753  0.568285                   openai      nopre-nam-bio   \n",
       "12  0.620712  0.558793                     bm25  nopre-nonam-nobio   \n",
       "23  0.618952  0.531911                    tfidf      nopre-nam-bio   \n",
       "1   0.618600  0.571916                   openai    pre-nonam-nobio   \n",
       "17  0.611667  0.491801                    tfidf  nopre-nonam-nobio   \n",
       "10  0.602466  0.532471             crossencoder        pre-nam-bio   \n",
       "5   0.602172  0.504463                    tfidf    pre-nonam-nobio   \n",
       "11  0.598374  0.538183                    tfidf        pre-nam-bio   \n",
       "2   0.597218  0.552529     sent-transformers-hf    pre-nonam-nobio   \n",
       "3   0.596751  0.554987  sent-transformers-local    pre-nonam-nobio   \n",
       "15  0.596546  0.563793  sent-transformers-local  nopre-nonam-nobio   \n",
       "14  0.591654  0.548149     sent-transformers-hf  nopre-nonam-nobio   \n",
       "20  0.588561  0.560076     sent-transformers-hf      nopre-nam-bio   \n",
       "21  0.584392  0.534687  sent-transformers-local      nopre-nam-bio   \n",
       "16  0.577275  0.538750             crossencoder  nopre-nonam-nobio   \n",
       "9   0.568142  0.520372  sent-transformers-local        pre-nam-bio   \n",
       "8   0.554250  0.507555     sent-transformers-hf        pre-nam-bio   \n",
       "22  0.529836  0.466154             crossencoder      nopre-nam-bio   \n",
       "\n",
       "       Time (s)  \n",
       "6      2.881830  \n",
       "4    681.923324  \n",
       "18     3.133084  \n",
       "0      2.066375  \n",
       "7    183.755365  \n",
       "13   182.610973  \n",
       "19   165.777771  \n",
       "12     2.256596  \n",
       "23     1.360650  \n",
       "1    143.422597  \n",
       "17     1.063158  \n",
       "10   926.972076  \n",
       "5      0.970865  \n",
       "11     1.217622  \n",
       "2    303.729978  \n",
       "3     20.212909  \n",
       "15    25.425695  \n",
       "14   394.124445  \n",
       "20   554.409397  \n",
       "21    35.194071  \n",
       "16  1280.249461  \n",
       "9     25.702166  \n",
       "8    481.835307  \n",
       "22  2290.554706  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for selected_variation in selected_variations:\n",
    "    dataset: AuredDataset = dataset_variations_dict[selected_variation]\n",
    "    for retriever_label in retrievers:\n",
    "        start = time.time()\n",
    "\n",
    "        retrieved_data = retrieve_evidence(dataset[:], retrievers[retriever_label])\n",
    "\n",
    "        pred = pd.DataFrame([[*d, retriever_label] for d in retrieved_data], columns=['qid', 'docno', 'rank', 'score', 'name']) \n",
    "\n",
    "        eval = ptpipelines.Evaluate(pred, golden, metrics = [R@5,MAP], perquery=False)\n",
    "        r5, meanap = [v for v in eval.values()]\n",
    "\n",
    "        score = r5\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        print(f'result for retrieval run - R@5: {r5:.4f} MAP: {meanap:.4f} with config\\tretriever: {retriever_label};\\tds: {selected_variation}, took {wall_time:.2f} seconds')\n",
    "        \n",
    "        data.append({\n",
    "            'R5': r5,\n",
    "            'MAP': meanap,\n",
    "            'Retrieval_Method': retriever_label, \n",
    "            'DS_Settings': selected_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_retrieval = pd.DataFrame(data)\n",
    "\n",
    "df_retrieval.to_csv(f'{out_dir}/df_retrieval.csv')\n",
    "print(f'saved df to {out_dir}/df_retrieval.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_retrieval.sort_values(by='R5', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
