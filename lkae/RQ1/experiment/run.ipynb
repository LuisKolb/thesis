{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.retrieval.retrieve import get_retriever, retrieve_evidence, AuredDataset\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, root_dir\n",
    "\n",
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "# walk through the pkl directory and load all the datasets in one of its subdirectories\n",
    "# load each dataset with its subdirectory name and filename as the key\n",
    "# skip non-pkl files\n",
    "for subdir in os.listdir(pkl_dir):\n",
    "    if not os.path.isdir(os.path.join(pkl_dir, subdir)):\n",
    "        continue            \n",
    "    datasets[subdir] = {}\n",
    "    for filename in os.listdir(os.path.join(pkl_dir, subdir)):\n",
    "        if not filename.endswith('.pkl'):\n",
    "            continue\n",
    "        key = os.path.join(subdir, filename)\n",
    "        datasets[subdir][filename.split('.')[0]] = load_pkl(os.path.join(pkl_dir, key))\n",
    "\n",
    "split = 'dev'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "# ground truth RQ1\n",
    "golden = ptio.read_qrels(os.path.join(root_dir, 'data', qrel_filename))\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"pre-nonam-nobio\", \"pre-nam-bio\", \"nopre-nonam-nobio\", \"nopre-nam-bio\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HFSentenceTransformersRetriever with model: sentence-transformers/multi-qa-distilbert-cos-v1\n",
      "Initializing CrossEncoderRetriever with model: cross-encoder/stsb-roberta-large\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bm25': <lkae.retrieval.methods.bm25.BM25Retriever at 0x23348d50430>,\n",
       " 'ollama': <lkae.retrieval.methods.ollama_embeddings.OllamaEmbeddingRetriever at 0x23348d50850>,\n",
       " 'openai': <lkae.retrieval.methods.openai_embeddings.OpenAIRetriever at 0x2339d4933a0>,\n",
       " 'sent-transformers-hf': <lkae.retrieval.methods.sent_transformers_hf.HFSentenceTransformersRetriever at 0x2339d4ac220>,\n",
       " 'sent-transformers-local': <lkae.retrieval.methods.sent_transformers.SBERTRetriever at 0x2339d4ad120>,\n",
       " 'crossencoder': <lkae.retrieval.methods.sent_transformers.CrossEncoderRetriever at 0x233aa1e6b90>,\n",
       " 'tfidf': <lkae.retrieval.methods.tfidf.TFIDFRetriever at 0x233aacebeb0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "\n",
    "retrievers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        retriever_label = get_retriever(**config)\n",
    "        retrievers[config['retriever_method']] = retriever_label\n",
    "\n",
    "retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for retrieval run - R@5: 0.7214 MAP: 0.6454 with config\tretriever: bm25;\tds: pre-nonam-nobio, took 0.47 seconds\n",
      "result for retrieval run - R@5: 0.4491 MAP: 0.3228 with config\tretriever: ollama;\tds: pre-nonam-nobio, took 341.44 seconds\n",
      "result for retrieval run - R@5: 0.7102 MAP: 0.6415 with config\tretriever: openai;\tds: pre-nonam-nobio, took 43.71 seconds\n",
      "result for retrieval run - R@5: 0.6642 MAP: 0.5380 with config\tretriever: sent-transformers-hf;\tds: pre-nonam-nobio, took 68.46 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisk\\miniconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for retrieval run - R@5: 0.6839 MAP: 0.5975 with config\tretriever: sent-transformers-local;\tds: pre-nonam-nobio, took 5.22 seconds\n",
      "result for retrieval run - R@5: 0.7018 MAP: 0.5909 with config\tretriever: crossencoder;\tds: pre-nonam-nobio, took 172.35 seconds\n",
      "result for retrieval run - R@5: 0.7235 MAP: 0.6261 with config\tretriever: tfidf;\tds: pre-nonam-nobio, took 0.23 seconds\n",
      "result for retrieval run - R@5: 0.7674 MAP: 0.6698 with config\tretriever: bm25;\tds: pre-nam-bio, took 0.61 seconds\n",
      "result for retrieval run - R@5: 0.3221 MAP: 0.2339 with config\tretriever: ollama;\tds: pre-nam-bio, took 401.76 seconds\n",
      "result for retrieval run - R@5: 0.7081 MAP: 0.6282 with config\tretriever: openai;\tds: pre-nam-bio, took 50.83 seconds\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for retrieval run - R@5: 0.5347 MAP: 0.4939 with config\tretriever: sent-transformers-hf;\tds: pre-nam-bio, took 108.11 seconds\n",
      "result for retrieval run - R@5: 0.6860 MAP: 0.6004 with config\tretriever: sent-transformers-local;\tds: pre-nam-bio, took 5.80 seconds\n",
      "result for retrieval run - R@5: 0.6621 MAP: 0.5704 with config\tretriever: crossencoder;\tds: pre-nam-bio, took 224.39 seconds\n",
      "result for retrieval run - R@5: 0.6818 MAP: 0.6364 with config\tretriever: tfidf;\tds: pre-nam-bio, took 0.28 seconds\n",
      "result for retrieval run - R@5: 0.6688 MAP: 0.6085 with config\tretriever: bm25;\tds: nopre-nonam-nobio, took 0.50 seconds\n",
      "result for retrieval run - R@5: 0.5105 MAP: 0.4289 with config\tretriever: ollama;\tds: nopre-nonam-nobio, took 377.57 seconds\n",
      "result for retrieval run - R@5: 0.7474 MAP: 0.6743 with config\tretriever: openai;\tds: nopre-nonam-nobio, took 47.41 seconds\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for retrieval run - R@5: 0.6400 MAP: 0.5428 with config\tretriever: sent-transformers-hf;\tds: nopre-nonam-nobio, took 103.54 seconds\n",
      "result for retrieval run - R@5: 0.6860 MAP: 0.6465 with config\tretriever: sent-transformers-local;\tds: nopre-nonam-nobio, took 5.58 seconds\n",
      "result for retrieval run - R@5: 0.6316 MAP: 0.5756 with config\tretriever: crossencoder;\tds: nopre-nonam-nobio, took 309.97 seconds\n",
      "result for retrieval run - R@5: 0.6688 MAP: 0.5932 with config\tretriever: tfidf;\tds: nopre-nonam-nobio, took 0.24 seconds\n",
      "result for retrieval run - R@5: 0.7674 MAP: 0.6303 with config\tretriever: bm25;\tds: nopre-nam-bio, took 0.68 seconds\n",
      "result for retrieval run - R@5: 0.4382 MAP: 0.3329 with config\tretriever: ollama;\tds: nopre-nam-bio, took 461.08 seconds\n",
      "result for retrieval run - R@5: 0.7081 MAP: 0.6182 with config\tretriever: openai;\tds: nopre-nam-bio, took 59.82 seconds\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for retrieval run - R@5: 0.6509 MAP: 0.5835 with config\tretriever: sent-transformers-hf;\tds: nopre-nam-bio, took 151.32 seconds\n",
      "result for retrieval run - R@5: 0.6860 MAP: 0.6209 with config\tretriever: sent-transformers-local;\tds: nopre-nam-bio, took 7.57 seconds\n",
      "result for retrieval run - R@5: 0.5723 MAP: 0.5258 with config\tretriever: crossencoder;\tds: nopre-nam-bio, took 516.59 seconds\n",
      "result for retrieval run - R@5: 0.6796 MAP: 0.6139 with config\tretriever: tfidf;\tds: nopre-nam-bio, took 0.31 seconds\n",
      "saved df to results/df_retrieval.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R5</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Retrieval_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.767368</td>\n",
       "      <td>0.669789</td>\n",
       "      <td>bm25</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>0.614144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.767368</td>\n",
       "      <td>0.630316</td>\n",
       "      <td>bm25</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>0.679263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.674269</td>\n",
       "      <td>openai</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>47.405012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.723509</td>\n",
       "      <td>0.626140</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.228726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.721404</td>\n",
       "      <td>0.645351</td>\n",
       "      <td>bm25</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.468739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.710175</td>\n",
       "      <td>0.641462</td>\n",
       "      <td>openai</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>43.710936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.708070</td>\n",
       "      <td>0.618158</td>\n",
       "      <td>openai</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>59.823467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.708070</td>\n",
       "      <td>0.628246</td>\n",
       "      <td>openai</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>50.831173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.590936</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>172.348049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.685965</td>\n",
       "      <td>0.620906</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>7.574176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.685965</td>\n",
       "      <td>0.600439</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>5.804895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.685965</td>\n",
       "      <td>0.646491</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>5.582420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.683860</td>\n",
       "      <td>0.597474</td>\n",
       "      <td>sent-transformers-local</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>5.220639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.681754</td>\n",
       "      <td>0.636351</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>0.276048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.679649</td>\n",
       "      <td>0.613947</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>0.310799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.668772</td>\n",
       "      <td>0.593158</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.241102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.668772</td>\n",
       "      <td>0.608509</td>\n",
       "      <td>bm25</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.498508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.664211</td>\n",
       "      <td>0.538041</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>68.462394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.662105</td>\n",
       "      <td>0.570374</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>224.391451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.650877</td>\n",
       "      <td>0.583480</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>151.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.542778</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>103.543102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.575585</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>309.968793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.572281</td>\n",
       "      <td>0.525760</td>\n",
       "      <td>crossencoder</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>516.585641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.534737</td>\n",
       "      <td>0.493947</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>108.109599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.510526</td>\n",
       "      <td>0.428947</td>\n",
       "      <td>ollama</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>377.568477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.449123</td>\n",
       "      <td>0.322807</td>\n",
       "      <td>ollama</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>341.438588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.438246</td>\n",
       "      <td>0.332854</td>\n",
       "      <td>ollama</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>461.080392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.322105</td>\n",
       "      <td>0.233860</td>\n",
       "      <td>ollama</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>401.760083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          R5       MAP         Retrieval_Method        DS_Settings    Time (s)\n",
       "7   0.767368  0.669789                     bm25        pre-nam-bio    0.614144\n",
       "21  0.767368  0.630316                     bm25      nopre-nam-bio    0.679263\n",
       "16  0.747368  0.674269                   openai  nopre-nonam-nobio   47.405012\n",
       "6   0.723509  0.626140                    tfidf    pre-nonam-nobio    0.228726\n",
       "0   0.721404  0.645351                     bm25    pre-nonam-nobio    0.468739\n",
       "2   0.710175  0.641462                   openai    pre-nonam-nobio   43.710936\n",
       "23  0.708070  0.618158                   openai      nopre-nam-bio   59.823467\n",
       "9   0.708070  0.628246                   openai        pre-nam-bio   50.831173\n",
       "5   0.701754  0.590936             crossencoder    pre-nonam-nobio  172.348049\n",
       "25  0.685965  0.620906  sent-transformers-local      nopre-nam-bio    7.574176\n",
       "11  0.685965  0.600439  sent-transformers-local        pre-nam-bio    5.804895\n",
       "18  0.685965  0.646491  sent-transformers-local  nopre-nonam-nobio    5.582420\n",
       "4   0.683860  0.597474  sent-transformers-local    pre-nonam-nobio    5.220639\n",
       "13  0.681754  0.636351                    tfidf        pre-nam-bio    0.276048\n",
       "27  0.679649  0.613947                    tfidf      nopre-nam-bio    0.310799\n",
       "20  0.668772  0.593158                    tfidf  nopre-nonam-nobio    0.241102\n",
       "14  0.668772  0.608509                     bm25  nopre-nonam-nobio    0.498508\n",
       "3   0.664211  0.538041     sent-transformers-hf    pre-nonam-nobio   68.462394\n",
       "12  0.662105  0.570374             crossencoder        pre-nam-bio  224.391451\n",
       "24  0.650877  0.583480     sent-transformers-hf      nopre-nam-bio  151.319000\n",
       "17  0.640000  0.542778     sent-transformers-hf  nopre-nonam-nobio  103.543102\n",
       "19  0.631579  0.575585             crossencoder  nopre-nonam-nobio  309.968793\n",
       "26  0.572281  0.525760             crossencoder      nopre-nam-bio  516.585641\n",
       "10  0.534737  0.493947     sent-transformers-hf        pre-nam-bio  108.109599\n",
       "15  0.510526  0.428947                   ollama  nopre-nonam-nobio  377.568477\n",
       "1   0.449123  0.322807                   ollama    pre-nonam-nobio  341.438588\n",
       "22  0.438246  0.332854                   ollama      nopre-nam-bio  461.080392\n",
       "8   0.322105  0.233860                   ollama        pre-nam-bio  401.760083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for selected_variation in selected_variations:\n",
    "    dataset: AuredDataset = dataset_variations_dict[selected_variation]\n",
    "    for retriever_label in retrievers:\n",
    "        start = time.time()\n",
    "\n",
    "        retrieved_data = retrieve_evidence(dataset[:], retrievers[retriever_label])\n",
    "\n",
    "        pred = pd.DataFrame([[*d, retriever_label] for d in retrieved_data], columns=['qid', 'docno', 'rank', 'score', 'name']) \n",
    "\n",
    "        eval = ptpipelines.Evaluate(pred, golden, metrics = [R@5,MAP], perquery=False)\n",
    "        r5, meanap = [v for v in eval.values()]\n",
    "\n",
    "        score = r5\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        print(f'result for retrieval run - R@5: {r5:.4f} MAP: {meanap:.4f} with config\\tretriever: {retriever_label};\\tds: {selected_variation}, took {wall_time:.2f} seconds')\n",
    "        \n",
    "        data.append({\n",
    "            'R5': r5,\n",
    "            'MAP': meanap,\n",
    "            'Retrieval_Method': retriever_label, \n",
    "            'DS_Settings': selected_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_retrieval = pd.DataFrame(data)\n",
    "\n",
    "df_retrieval.to_csv(f'{out_dir}/df_retrieval.csv')\n",
    "print(f'saved df to {out_dir}/df_retrieval.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_retrieval.sort_values(by='R5', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
