{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.10 (built by craigm on 2024-08-22 17:33) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkls, root_dir, AuredDataset\n",
    "from lkae.retrieval.retrieve import get_retriever, retrieve_evidence, AuredDataset\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'dev'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ1\n",
    "golden = ptio.read_qrels(os.path.join(root_dir, 'data', qrel_filename))\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "# these selected variations are selected for these reasons:\n",
    "# - pre-nonam-nobio     (\"raw\" data, but preprocessed)\n",
    "# - pre-nam-bio         (we would expect lexical retrieval to be best here)\n",
    "# - nopre-nonam-nobio   (\"raw\" data)\n",
    "# - nopre-nam-bio       (we would expect semantic retrieval to be best here, most information contained here)\n",
    "selected_variations = [\"pre-nonam-nobio\", \"nopre-nonam-nobio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bm25': <lkae.retrieval.methods.bm25.BM25Retriever at 0x20b0483e530>,\n",
       " 'rerank-nv-embed-v1': <lkae.retrieval.methods.rerank_bm25_nv.RerankingRetriever at 0x20b0483f730>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "\n",
    "retrievers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        retriever_label = get_retriever(**config)\n",
    "        retrievers[config['retriever_method']] = retriever_label\n",
    "\n",
    "retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for retrieval run - R@5: 0.7214 MAP: 0.6454 with config\tretriever: bm25;\tds: pre-nonam-nobio, took 0.58 seconds\n",
      "result for retrieval run - R@5: 0.8242 MAP: 0.7840 with config\tretriever: rerank-nv-embed-v1;\tds: pre-nonam-nobio, took 140.10 seconds\n",
      "result for retrieval run - R@5: 0.6688 MAP: 0.6085 with config\tretriever: bm25;\tds: nopre-nonam-nobio, took 0.50 seconds\n",
      "result for retrieval run - R@5: 0.8242 MAP: 0.8016 with config\tretriever: rerank-nv-embed-v1;\tds: nopre-nonam-nobio, took 143.98 seconds\n",
      "saved df to results/df_retrieval.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R5</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Retrieval_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.824211</td>\n",
       "      <td>0.784023</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>140.100415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.824211</td>\n",
       "      <td>0.801567</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>143.983665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.721404</td>\n",
       "      <td>0.645351</td>\n",
       "      <td>bm25</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.584160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.668772</td>\n",
       "      <td>0.608509</td>\n",
       "      <td>bm25</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.495882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         R5       MAP    Retrieval_Method        DS_Settings    Time (s)\n",
       "1  0.824211  0.784023  rerank-nv-embed-v1    pre-nonam-nobio  140.100415\n",
       "3  0.824211  0.801567  rerank-nv-embed-v1  nopre-nonam-nobio  143.983665\n",
       "0  0.721404  0.645351                bm25    pre-nonam-nobio    0.584160\n",
       "2  0.668772  0.608509                bm25  nopre-nonam-nobio    0.495882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for selected_variation in selected_variations:\n",
    "    dataset: AuredDataset = dataset_variations_dict[selected_variation]\n",
    "    for retriever_label in retrievers:\n",
    "        start = time.time()\n",
    "\n",
    "        retrieved_data = retrieve_evidence(dataset[:], retrievers[retriever_label])\n",
    "\n",
    "        pred = pd.DataFrame([[*d, retriever_label] for d in retrieved_data], columns=['qid', 'docno', 'rank', 'score', 'name']) \n",
    "\n",
    "        eval = ptpipelines.Evaluate(pred, golden, metrics = [R@5,MAP], perquery=False)\n",
    "        r5, meanap = [v for v in eval.values()]\n",
    "\n",
    "        score = r5\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        print(f'result for retrieval run - R@5: {r5:.4f} MAP: {meanap:.4f} with config\\tretriever: {retriever_label};\\tds: {selected_variation}, took {wall_time:.2f} seconds')\n",
    "        \n",
    "        data.append({\n",
    "            'R5': r5,\n",
    "            'MAP': meanap,\n",
    "            'Retrieval_Method': retriever_label, \n",
    "            'DS_Settings': selected_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_retrieval = pd.DataFrame(data)\n",
    "\n",
    "df_retrieval.to_csv(f'{out_dir}/df_retrieval.csv')\n",
    "print(f'saved df to {out_dir}/df_retrieval.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_retrieval.sort_values(by='R5', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
