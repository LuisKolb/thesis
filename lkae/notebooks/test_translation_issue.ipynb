{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from lkae.utils.data_loading import AuredDataset, root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rumor AuRED_090 has non-empty evidence array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'English_train.jsonl-AuRED_052': {'issue_perc': 44.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_058': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_063': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_074': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_075': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_081': {'issue_perc': 73.3, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_069': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_090': {'issue_perc': 0.6, 'has_ev': True},\n",
       " 'English_train.jsonl-AuRED_071': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_061': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_train.jsonl-AuRED_055': {'issue_perc': 100.0, 'has_ev': False}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'English_dev.jsonl-AuRED_066': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_dev.jsonl-AuRED_076': {'issue_perc': 96.7, 'has_ev': False}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'English_test.jsonl-AuRED_092': {'issue_perc': 0.9, 'has_ev': False},\n",
       " 'English_test.jsonl-AuRED_054': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_test.jsonl-AuRED_042': {'issue_perc': 1.4, 'has_ev': False},\n",
       " 'English_test.jsonl-AuRED_060': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_test.jsonl-AuRED_070': {'issue_perc': 100.0, 'has_ev': False},\n",
       " 'English_test.jsonl-AuRED_056': {'issue_perc': 100.0, 'has_ev': False}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_names = ['English_train.jsonl', 'English_dev.jsonl', 'English_test.jsonl']\n",
    "\n",
    "blacklist = []\n",
    "maybelist = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    issue_dict = {} # issues per id\n",
    "    config = {\n",
    "        'preprocess': False,\n",
    "        'add_author_name': False,\n",
    "        'add_author_bio': False,\n",
    "        'author_info_filepath': os.path.join(root_dir, 'data', 'combined-author-data-translated.json'),\n",
    "    }\n",
    "\n",
    "    fingerprint = 'pre-' if config['preprocess'] else 'nopre-'\n",
    "    fingerprint += 'nam-' if config['add_author_name'] else 'nonam-'\n",
    "    fingerprint += 'bio' if config['add_author_bio'] else 'nobio'\n",
    "\n",
    "    ds = AuredDataset(os.path.join(root_dir, 'data', file_name), **config)\n",
    "\n",
    "    for rumor in ds:\n",
    "        id = rumor['id']\n",
    "        timeline = rumor['timeline']\n",
    "        total_issues = 0\n",
    "\n",
    "        for tweet in timeline:\n",
    "            if \"ISSUE: couldn't translate\" in tweet[2]:\n",
    "                total_issues += 1\n",
    "\n",
    "        # filter down to tweets with transl issues\n",
    "        if total_issues > 0:\n",
    "            \n",
    "            has_evidence = False\n",
    "            # test for tweets with translation issues that have \n",
    "            if 'evidence' in rumor and rumor['evidence'] and len(rumor['evidence']) > 0:\n",
    "                print(f'rumor {id} has non-empty evidence array')\n",
    "                has_evidence = True\n",
    "                for ev in rumor['evidence']:\n",
    "\n",
    "                    # any evidence with translation issues?\n",
    "                    # if not, we could just cull tweets from the tl with transl issues...\n",
    "                    # ... as the tweet would be verifiable without those tweets \n",
    "                    if \"ISSUE: couldn't translate\" in ev[2]:\n",
    "                        print(f'OH NO! transl issue in evidence for {id}')\n",
    "\n",
    "            # calculate % of timeline tweets that have issue\n",
    "            issue_percent = round((total_issues/len(timeline))*100, 1)\n",
    "            if issue_percent == 100.0:\n",
    "                blacklist.append(id)\n",
    "            else:\n",
    "                maybelist.append(id)\n",
    "            issue_dict[f\"{file_name}-{id}\"] = {'issue_perc': issue_percent, 'has_ev': has_evidence}\n",
    "\n",
    "    display(issue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AuRED_058',\n",
       " 'AuRED_063',\n",
       " 'AuRED_074',\n",
       " 'AuRED_075',\n",
       " 'AuRED_069',\n",
       " 'AuRED_071',\n",
       " 'AuRED_061',\n",
       " 'AuRED_055',\n",
       " 'AuRED_066',\n",
       " 'AuRED_054',\n",
       " 'AuRED_060',\n",
       " 'AuRED_070',\n",
       " 'AuRED_056']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we will definitely discard all rumors that have timelines ith 100% translation issues \n",
    "display(blacklist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AuRED_052', 'AuRED_081', 'AuRED_090', 'AuRED_076', 'AuRED_092', 'AuRED_042']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# maybe keep those rumors? it's only 6 though (over all datasets)\n",
    "display(maybelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AuRED_058',\n",
       " 'AuRED_063',\n",
       " 'AuRED_074',\n",
       " 'AuRED_075',\n",
       " 'AuRED_069',\n",
       " 'AuRED_071',\n",
       " 'AuRED_061',\n",
       " 'AuRED_055',\n",
       " 'AuRED_066',\n",
       " 'AuRED_054',\n",
       " 'AuRED_060',\n",
       " 'AuRED_070',\n",
       " 'AuRED_056',\n",
       " 'AuRED_052',\n",
       " 'AuRED_081',\n",
       " 'AuRED_090',\n",
       " 'AuRED_076',\n",
       " 'AuRED_092',\n",
       " 'AuRED_042']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cull_list = [*blacklist, *maybelist]\n",
    "cull_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
