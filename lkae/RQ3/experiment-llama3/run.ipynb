{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.10 (built by craigm on 2024-08-22 17:33) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkls, root_dir, AuredDataset\n",
    "from lkae.retrieval.retrieve import get_retriever,retrieve_evidence\n",
    "from lkae.verification.verify import get_verifier, Judge, run_verifier_on_dataset\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ3\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"nopre-nam-bio\", \"nopre-nonam-nobio\", \"pre-nam-bio\", \"pre-nonam-nobio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rerank-nv-embed-v1__llama3-1-8b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_azure_ai.Llama3AzureVerifier at 0x1d91df006a0>},\n",
       " 'rerank-nv-embed-v1__llama3-1-70b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_azure_ai.Llama3AzureVerifier at 0x1d91df03e50>},\n",
       " 'rerank-nv-embed-v1__llama3-1-405b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_azure_ai.Llama3AzureVerifier at 0x1d91df00d90>}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "setups = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        exp_fingerprint = f'{config[\"retriever_method\"]}__{config[\"verifier_method\"]}'\n",
    "        \n",
    "        # retriever = get_retriever(**config)\n",
    "        verifier = get_verifier(**config)\n",
    "        \n",
    "        setups[exp_fingerprint] = {}\n",
    "        setups[exp_fingerprint]['retriever'] = config[\"retriever_method\"]\n",
    "        setups[exp_fingerprint]['verifier'] = verifier\n",
    "\n",
    "display(setups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nam-bio_rerank-nv-embed-v1__llama3-1-8b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.5172 Strict-Macro-F1: 0.5108 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nam-bio_rerank-nv-embed-v1__llama3-1-70b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7218 Strict-Macro-F1: 0.7090 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nam-bio_rerank-nv-embed-v1__llama3-1-405b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7041 Strict-Macro-F1: 0.6887 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-405b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__llama3-1-8b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.5882 Strict-Macro-F1: 0.5767 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__llama3-1-70b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7765 Strict-Macro-F1: 0.7688 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-1-405b on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba7755dba9a4501bc2812124cb20961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8}\n",
      "\n",
      "Reasoning: The statement from the Yemeni Ministry of Foreign Affairs mentions that the Minister discussed the possibility of evacuation of Yemeni nationals, including students, from Wuhan, China, and the sending of additional financial aid. This implies that the Yemeni government is taking steps to help its citizens in China, which contradicts the claim that Saudi Arabia did not evacuate Yemeni students while Oman did. However, the statement does not directly address the specific claim about Saudi Arabia's actions, hence the confidence score is not 1., returning NOT ENOUGH INFO answer\n",
      "could not json-parse response from Azure API: {\"decision\": \"NOT ENOUGH INFO\", \"confidence\": 0.8} \n",
      "\n",
      "The statement from the Chinese Embassy in Yemen only talks about the Chinese government providing food to Yemeni students in Wuhan, while the claim is about Saudi Arabia and Oman's actions regarding evacuating Yemeni students from China. There is no direct connection between the two, so the statement does not provide enough information to support or refute the claim., returning NOT ENOUGH INFO answer\n",
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8}\n",
      "\n",
      "Reasoning: The statement from the authority account \"CAF_Media\" mentions the teams participating in the Total CAF Super Cup, which are Al Ahly and RS Berkane, but does not mention anything about a person named \"Samar_Hosni\" or any controversy surrounding her. This suggests that the claim about CAF keeping her away from the final is likely false. However, the confidence score is not 1 because the statement does not explicitly deny the claim, it simply does not provide any information that supports it., returning NOT ENOUGH INFO answer\n",
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8}\n",
      "\n",
      "Reasoning: The statement from the authority account \"SerajSat\" mentions that Ismail Haniyeh congratulated the Taliban on their victory, whereas the claim states that Musa Abu Marzouq Al-Hamsawi congratulated the Taliban. Since the statement and the claim mention different individuals congratulating the Taliban, the statement refutes the claim. However, the confidence score is not 1 because the statement does not explicitly deny the claim, but rather provides contradictory information., returning NOT ENOUGH INFO answer\n",
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8}\n",
      "\n",
      "Reasoning: The statement from the Authority Account \"MofaQatar_AR\" mentions a meeting between the Deputy Prime Minister and Minister of Foreign Affairs of Qatar and a delegation from the Taliban, which suggests a diplomatic or official meeting. This refutes the claim that the meeting was a \"meeting of the terrorists\" and that Qatar is \"the financier of the terrorist organization\", as the tone and context of the statement suggest a legitimate and official interaction. However, the confidence score is not 1 because the statement does not directly address the claim about Musa Abu Marzouq Al-Hamsawi congratulating the Taliban, and the tone of the claim is also quite different from the tone of the statement., returning NOT ENOUGH INFO answer\n",
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8}\n",
      "\n",
      "Reasoning: The statement from the Authority Account \"MofaQatar_AR\" mentions a meeting between the Deputy Prime Minister and Minister of Foreign Affairs of Qatar and the head of the Taliban's political office, which suggests a diplomatic engagement. In contrast, the claim implies that Qatar is financing a terrorist organization (the Taliban) and that a meeting took place between terrorists (the Taliban) and Haniyeh. The tone and content of the statement and the claim are contradictory, suggesting that the statement refutes the claim. However, the confidence score is not 1 because the statement does not directly address the specific allegations made in the claim., returning NOT ENOUGH INFO answer\n",
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8}\n",
      "\n",
      "Reasoning: The statement from MBA_AlThani_ mentions a meeting with a Taliban delegation to discuss a peaceful transfer of power and stability in Afghanistan, which contradicts the claim that the meeting was to congratulate the Taliban on their victory. The tone and content of the two statements are also quite different, suggesting that they are not related in a supportive manner. However, I'm not 100% confident as the statements are from different sources and may not be directly related, hence the 0.8 confidence score., returning NOT ENOUGH INFO answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t173243\n",
      "prompt tokens:\t162877\n",
      "completion tokens:\t10366\n",
      "price estimate:\t$0.8798337\n",
      "result for verification run - Macro-F1: 0.7409 Strict-Macro-F1: 0.7300 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-405b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-1-8b on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9eed19db4b476cb903bc0c18a7d6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t171340\n",
      "prompt tokens:\t161743\n",
      "completion tokens:\t9597\n",
      "price estimate:\t$0.05437706999999999\n",
      "result for verification run - Macro-F1: 0.6158 Strict-Macro-F1: 0.6052 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-1-70b on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54feffe85b040da805d03202ea800e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t171379\n",
      "prompt tokens:\t161743\n",
      "completion tokens:\t9636\n",
      "price estimate:\t$0.46758268\n",
      "result for verification run - Macro-F1: 0.7052 Strict-Macro-F1: 0.6923 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-1-405b on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7fb3dceb1844088cf563c77d456caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t344658\n",
      "prompt tokens:\t324620\n",
      "completion tokens:\t20038\n",
      "price estimate:\t$1.7525468\n",
      "result for verification run - Macro-F1: 0.7047 Strict-Macro-F1: 0.6894 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-405b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/pre-nonam-nobio_rerank-nv-embed-v1__llama3-1-8b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.6525 Strict-Macro-F1: 0.6417 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/pre-nonam-nobio_rerank-nv-embed-v1__llama3-1-70b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7315 Strict-Macro-F1: 0.7156 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/pre-nonam-nobio_rerank-nv-embed-v1__llama3-1-405b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7735 Strict-Macro-F1: 0.7548 with retriever: rerank-nv-embed-v1 and retriever: llama3-1-405b\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Retrieval_Method</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.776465</td>\n",
       "      <td>0.768775</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-70b</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.773538</td>\n",
       "      <td>0.754840</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-405b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.017059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.740946</td>\n",
       "      <td>0.730040</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-405b</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>951.188266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.731470</td>\n",
       "      <td>0.715628</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-70b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.017273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.721766</td>\n",
       "      <td>0.709036</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-70b</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>0.015060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.705238</td>\n",
       "      <td>0.692306</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-70b</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>536.397683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.704749</td>\n",
       "      <td>0.689438</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-405b</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>1018.201312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.704116</td>\n",
       "      <td>0.688744</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-405b</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>0.017540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.652548</td>\n",
       "      <td>0.641708</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-8b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.030309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.615837</td>\n",
       "      <td>0.605200</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-8b</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>226.308516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588212</td>\n",
       "      <td>0.576674</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-8b</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.023654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517222</td>\n",
       "      <td>0.510842</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-1-8b</td>\n",
       "      <td>nopre-nam-bio</td>\n",
       "      <td>0.027740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Macro-F1  Strict-Macro-F1    Retrieval_Method Verifier_Method  \\\n",
       "4   0.776465         0.768775  rerank-nv-embed-v1    llama3-1-70b   \n",
       "11  0.773538         0.754840  rerank-nv-embed-v1   llama3-1-405b   \n",
       "5   0.740946         0.730040  rerank-nv-embed-v1   llama3-1-405b   \n",
       "10  0.731470         0.715628  rerank-nv-embed-v1    llama3-1-70b   \n",
       "1   0.721766         0.709036  rerank-nv-embed-v1    llama3-1-70b   \n",
       "7   0.705238         0.692306  rerank-nv-embed-v1    llama3-1-70b   \n",
       "8   0.704749         0.689438  rerank-nv-embed-v1   llama3-1-405b   \n",
       "2   0.704116         0.688744  rerank-nv-embed-v1   llama3-1-405b   \n",
       "9   0.652548         0.641708  rerank-nv-embed-v1     llama3-1-8b   \n",
       "6   0.615837         0.605200  rerank-nv-embed-v1     llama3-1-8b   \n",
       "3   0.588212         0.576674  rerank-nv-embed-v1     llama3-1-8b   \n",
       "0   0.517222         0.510842  rerank-nv-embed-v1     llama3-1-8b   \n",
       "\n",
       "          DS_Settings     Time (s)  \n",
       "4   nopre-nonam-nobio     0.014027  \n",
       "11    pre-nonam-nobio     0.017059  \n",
       "5   nopre-nonam-nobio   951.188266  \n",
       "10    pre-nonam-nobio     0.017273  \n",
       "1       nopre-nam-bio     0.015060  \n",
       "7         pre-nam-bio   536.397683  \n",
       "8         pre-nam-bio  1018.201312  \n",
       "2       nopre-nam-bio     0.017540  \n",
       "9     pre-nonam-nobio     0.030309  \n",
       "6         pre-nam-bio   226.308516  \n",
       "3   nopre-nonam-nobio     0.023654  \n",
       "0       nopre-nam-bio     0.027740  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "\n",
    "for dataset_variation in selected_variations:\n",
    "\n",
    "    for exp_fingerprint in setups:\n",
    "        # get the dataset here since it is modified in place here, contrary to RQ2\n",
    "        dataset: AuredDataset = dataset_variations_dict[dataset_variation]\n",
    "        start = time.time()\n",
    "\n",
    "        # retrieved_data = retrieve_evidence(dataset, setups[exp_fingerprint]['retriever'])\n",
    "        data_path = f'{root_dir}/RQ1/experiment-{split}/results/{dataset_variation}_{setups[exp_fingerprint][\"retriever\"]}.pkl'\n",
    "        retrieved_data = pkl.load(open(data_path, 'rb'))\n",
    "        print(f'loaded retrieval results from {data_path}')\n",
    "\n",
    "        dataset.add_trec_list_judgements(retrieved_data)\n",
    "\n",
    "        run_filename = f'{out_dir}/{dataset_variation}_{exp_fingerprint}.pkl'\n",
    "\n",
    "        # check if the file already exists from a previous run\n",
    "        if os.path.exists(run_filename):\n",
    "            print(f'found {run_filename}, loading from file')\n",
    "            verification_results = pkl.load(open(run_filename, 'rb'))\n",
    "        else:\n",
    "            print(f'running {exp_fingerprint} on {dataset_variation}')\n",
    "            verification_results = run_verifier_on_dataset(\n",
    "                dataset=dataset,\n",
    "                verifier=setups[exp_fingerprint]['verifier'],\n",
    "                judge=solomon,\n",
    "                blind=False,\n",
    "            )\n",
    "            pkl.dump(verification_results, open(run_filename, 'wb'))\n",
    "\n",
    "        # print(verification_results)\n",
    "\n",
    "        macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "        retriever_label, verifier_label = exp_fingerprint.split('__')\n",
    "\n",
    "        print(\n",
    "            f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with retriever: {retriever_label} and retriever: {verifier_label}\"\n",
    "        )\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        data.append({\n",
    "            'Macro-F1': macro_f1,\n",
    "            'Strict-Macro-F1': strict_macro_f1,\n",
    "            'Retrieval_Method': retriever_label, \n",
    "            'Verifier_Method': verifier_label, \n",
    "            'DS_Settings': dataset_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
