{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, root_dir, AuredDataset, AuthorityPost\n",
    "from lkae.retrieval.retrieve import get_retriever,retrieve_evidence\n",
    "from lkae.verification.verify import get_verifier, Judge, run_verifier_on_dataset\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "\n",
    "# import pyterrier as pt\n",
    "# import pyterrier.io as ptio\n",
    "# import pyterrier.pipelines as ptpipelines\n",
    "# from ir_measures import R, MAP    \n",
    "\n",
    "# if not pt.started():\n",
    "#     pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "# walk through the pkl directory and load all the datasets in one of its subdirectories\n",
    "# load each dataset with its subdirectory name and filename as the key\n",
    "# skip non-pkl files\n",
    "for subdir in os.listdir(pkl_dir):\n",
    "    if not os.path.isdir(os.path.join(pkl_dir, subdir)):\n",
    "        continue            \n",
    "    datasets[subdir] = {}\n",
    "    for filename in os.listdir(os.path.join(pkl_dir, subdir)):\n",
    "        if not filename.endswith('.pkl'):\n",
    "            continue\n",
    "        key = os.path.join(subdir, filename)\n",
    "        datasets[subdir][filename.split('.')[0]] = load_pkl(os.path.join(pkl_dir, key))\n",
    "\n",
    "split = 'dev'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "# ground truth RQ3\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a single variation of the dataset\n",
    "selected_variation = \"pre-nonam-nobio\"\n",
    "dataset: AuredDataset = dataset_variations_dict[selected_variation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HFSentenceTransformersRetriever with model: sentence-transformers/multi-qa-distilbert-cos-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HFSentenceTransformersRetriever with model: sentence-transformers/multi-qa-distilbert-cos-v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bm25__transformers-roberta': {'retriever': <lkae.retrieval.methods.bm25.BM25Retriever at 0x21fe9192b30>,\n",
       "  'verifier': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x21fe9192bf0>},\n",
       " 'bm25__openai': {'retriever': <lkae.retrieval.methods.bm25.BM25Retriever at 0x21f8b12ebf0>,\n",
       "  'verifier': <lkae.verification.models.open_ai.OpenaiVerifier at 0x22028a4e0e0>},\n",
       " 'sent-transformers-hf__transformers-roberta': {'retriever': <lkae.retrieval.methods.sent_transformers_hf.HFSentenceTransformersRetriever at 0x2202ab75fc0>,\n",
       "  'verifier': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x2202ab76860>},\n",
       " 'sent-transformers-hf__openai': {'retriever': <lkae.retrieval.methods.sent_transformers_hf.HFSentenceTransformersRetriever at 0x2202ab76200>,\n",
       "  'verifier': <lkae.verification.models.open_ai.OpenaiVerifier at 0x2202ac91450>}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "setups = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        exp_fingerprint = f'{config[\"retriever_method\"]}__{config[\"verifier_method\"]}'\n",
    "        \n",
    "        retriever = get_retriever(**config)\n",
    "        verifier = get_verifier(**config)\n",
    "        \n",
    "        setups[exp_fingerprint] = {}\n",
    "        setups[exp_fingerprint]['retriever'] = retriever\n",
    "        setups[exp_fingerprint]['verifier'] = verifier\n",
    "\n",
    "display(setups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HFSentenceTransformersRetriever with model: sentence-transformers/multi-qa-distilbert-cos-v1\n",
      "Initializing HFSentenceTransformersRetriever with model: sentence-transformers/multi-qa-distilbert-cos-v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bm25': <lkae.retrieval.methods.bm25.BM25Retriever at 0x21fe9193250>,\n",
       " 'sent-transformers-hf': <lkae.retrieval.methods.sent_transformers_hf.HFSentenceTransformersRetriever at 0x21f8afd5000>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "retrievers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        retriever_label = get_retriever(**config)\n",
    "        retrievers[config['retriever_method']] = retriever_label\n",
    "\n",
    "retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(dataset):\n",
    "    retrieved_ev = []\n",
    "    evidences = item[\"evidence\"]\n",
    "    if evidences is None:\n",
    "        print(f\"skipped {i} because no evidence\")\n",
    "        continue\n",
    "    for ev in evidences:\n",
    "        retrieved_ev.append(AuthorityPost(ev.url, ev.post_id, ev.text, 1, 1))\n",
    "    dataset[i][\"retrieved_evidence\"] = retrieved_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.4361 Strict-Macro-F1: 0.4062 with config {'retriever_method': 'sent-transformers-hf', 'retriever_k': 5, 'retriever_model': 'sentence-transformers/multi-qa-distilbert-cos-v1', 'verifier_method': 'openai'}\n",
      "-----total token usage for verification-----\n",
      "total tokens:\t46556\n",
      "prompt tokens:\t44213\n",
      "completion tokens:\t2343\n",
      "price estimate:\t$0.51242\n",
      "result for verification run - Macro-F1: 0.8968 Strict-Macro-F1: 0.8827 with config {'retriever_method': 'sent-transformers-hf', 'retriever_k': 5, 'retriever_model': 'sentence-transformers/multi-qa-distilbert-cos-v1', 'verifier_method': 'openai'}\n",
      "Waiting for model to warm up (for 20.0 seconds)\n",
      "result for verification run - Macro-F1: 0.5628 Strict-Macro-F1: 0.5015 with config {'retriever_method': 'sent-transformers-hf', 'retriever_k': 5, 'retriever_model': 'sentence-transformers/multi-qa-distilbert-cos-v1', 'verifier_method': 'openai'}\n",
      "-----total token usage for verification-----\n",
      "total tokens:\t45910\n",
      "prompt tokens:\t43574\n",
      "completion tokens:\t2336\n",
      "price estimate:\t$0.50582\n",
      "result for verification run - Macro-F1: 0.8968 Strict-Macro-F1: 0.8799 with config {'retriever_method': 'sent-transformers-hf', 'retriever_k': 5, 'retriever_model': 'sentence-transformers/multi-qa-distilbert-cos-v1', 'verifier_method': 'openai'}\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Retrieval_Method</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.882716</td>\n",
       "      <td>bm25</td>\n",
       "      <td>openai</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>526.545298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.879917</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>openai</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>564.810798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.562802</td>\n",
       "      <td>0.501499</td>\n",
       "      <td>sent-transformers-hf</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>132.525914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.436147</td>\n",
       "      <td>0.406170</td>\n",
       "      <td>bm25</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>10.869552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Macro-F1  Strict-Macro-F1      Retrieval_Method       Verifier_Method  \\\n",
       "1  0.896825         0.882716                  bm25                openai   \n",
       "3  0.896825         0.879917  sent-transformers-hf                openai   \n",
       "2  0.562802         0.501499  sent-transformers-hf  transformers-roberta   \n",
       "0  0.436147         0.406170                  bm25  transformers-roberta   \n",
       "\n",
       "       DS_Settings    Time (s)  \n",
       "1  pre-nonam-nobio  526.545298  \n",
       "3  pre-nonam-nobio  564.810798  \n",
       "2  pre-nonam-nobio  132.525914  \n",
       "0  pre-nonam-nobio   10.869552  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "\n",
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for exp_fingerprint in setups:\n",
    "    start = time.time()\n",
    "\n",
    "    retrieved_data = retrieve_evidence(dataset, setups[exp_fingerprint]['retriever'])\n",
    "\n",
    "    dataset.add_trec_list_judgements(retrieved_data)\n",
    "\n",
    "    verification_results = run_verifier_on_dataset(\n",
    "        dataset=dataset,\n",
    "        verifier=setups[exp_fingerprint]['verifier'],\n",
    "        judge=solomon,\n",
    "        blind=False,\n",
    "    )\n",
    "\n",
    "    # print(verification_results)\n",
    "\n",
    "    macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "    print(\n",
    "        f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with config {config}\"\n",
    "    )\n",
    "\n",
    "    wall_time = time.time() - start\n",
    "\n",
    "    retriever, verifier = exp_fingerprint.split('__')\n",
    "    \n",
    "    data.append({\n",
    "        'Macro-F1': macro_f1,\n",
    "        'Strict-Macro-F1': strict_macro_f1,\n",
    "        'Retrieval_Method': retriever, \n",
    "        'Verifier_Method': verifier, \n",
    "        'DS_Settings': selected_variation,\n",
    "        'Time (s)': wall_time,\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
