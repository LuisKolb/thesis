{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.10 (built by craigm on 2024-08-22 17:33) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkls, root_dir, AuredDataset\n",
    "from lkae.retrieval.retrieve import get_retriever,retrieve_evidence\n",
    "from lkae.verification.verify import get_verifier, Judge, run_verifier_on_dataset\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ3\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"nopre-nonam-nobio\", \"pre-nam-bio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rerank-nv-embed-v1__llama3-8b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x1165c6c19c0>},\n",
       " 'rerank-nv-embed-v1__llama3-70b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x11687e53910>},\n",
       " 'rerank-nv-embed-v1__transformers-roberta': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x11687e7f880>},\n",
       " 'rerank-nv-embed-v1__transformers-bart': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x116907ee830>},\n",
       " 'rerank-nv-embed-v1__openai-4o-mini': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x116aa02faf0>},\n",
       " 'rerank-nv-embed-v1__openai-4o': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x116aa739ed0>}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "setups = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        exp_fingerprint = f'{config[\"retriever_method\"]}__{config[\"verifier_method\"]}'\n",
    "        \n",
    "        # retriever = get_retriever(**config)\n",
    "        verifier = get_verifier(**config)\n",
    "        \n",
    "        setups[exp_fingerprint] = {}\n",
    "        setups[exp_fingerprint]['retriever'] = config[\"retriever_method\"]\n",
    "        setups[exp_fingerprint]['verifier'] = verifier\n",
    "\n",
    "display(setups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__llama3-8b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.6702 Strict-Macro-F1: 0.6646 with retriever: rerank-nv-embed-v1 and retriever: llama3-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__llama3-70b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7547 Strict-Macro-F1: 0.7434 with retriever: rerank-nv-embed-v1 and retriever: llama3-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__transformers-roberta.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.3223 Strict-Macro-F1: 0.3223 with retriever: rerank-nv-embed-v1 and retriever: transformers-roberta\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__transformers-bart.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.2835 Strict-Macro-F1: 0.2751 with retriever: rerank-nv-embed-v1 and retriever: transformers-bart\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__openai-4o-mini.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.8039 Strict-Macro-F1: 0.7954 with retriever: rerank-nv-embed-v1 and retriever: openai-4o-mini\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "found results/nopre-nonam-nobio_rerank-nv-embed-v1__openai-4o.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.8502 Strict-Macro-F1: 0.8424 with retriever: rerank-nv-embed-v1 and retriever: openai-4o\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "found results/pre-nam-bio_rerank-nv-embed-v1__llama3-8b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.6280 Strict-Macro-F1: 0.6161 with retriever: rerank-nv-embed-v1 and retriever: llama3-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-70b on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67acdc7393654c1fac1429296ab1eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: REFUTES: 0.7\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.85}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: REFUTES, Confidence: 0.9\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.8}\n",
      "ERROR: could not decode answer from model: REFUTES, 1\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.6}\n",
      "ERROR: could not decode answer from model: REFUTES: 0.8\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: REFUTES, 0.65\n",
      "ERROR: could not decode answer from model: SUPPORTS, Confidence: 0.9\n",
      "ERROR: could not decode answer from model: {\"SUPPORTS\", confidence: 0.9}\n",
      "ERROR: could not decode answer from model: {\"SUPPORTS\", 0.98}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"NOT ENOUGH INFO\", 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", confidence: 0.95}\n",
      "ERROR: could not decode answer from model: NOT ENOUGH INFO, 0.67\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"SUPPORTS\", confidence: 0.9}\n",
      "ERROR: could not decode answer from model: REFUTES, Confidence: 0.7 \n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.8}\n",
      "ERROR: could not decode answer from model: SUPPORTS\n",
      "ERROR: could not decode answer from model: REFUTES, Confidence: 0.8\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.7}\n",
      "ERROR: could not decode answer from model: {\"NOT ENOUGH INFO\": 1}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1} is not correct and we cant use new information.\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1.0}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not decode answer from model: REFUTES \n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": \"0.9\"}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: REFUTES, Confidence: 1.0\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"claim\": REFUTES, \"confidence\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"claim\": \"By presidential order canceling the Corona test for Algerians at border crossings Algeria Border Crossings Tunisia Coronavirus\", \"statement\": \"Important: The President of the Republic Mr Abdelmadjid Tebboune honors the first and second best students nationally in the baccalaureate degree Ben Abbas Iman and Manar Merdas The honoring ceremony was attended by the Advisor to the President of the Republic in charge of Legal and Judicial Affairs the Acting Director of the Bureau Mr Boualem Bou\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1.0}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.5}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", confidence: 0.8}\n",
      "ERROR: could not decode answer from model: {\"claim\": \"The Embassy of Kuwait in Washington is a tourist attraction\", \"statement\": \"Authority: 'The Official Account of the Embassy of the State of Kuwait to the USA The official account of the Kuwaiti Embassy in Washington Emergency phones Statement: 'Over the holiday season, the embassy will have limited hours of operation\", \"result\": {\"decision\": \"NOT ENOUGH INFO\", \"confidence\": 0.8}}\n",
      "ERROR: could not decode answer from model: {\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", Confidence: 0.7}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.85}\n",
      "ERROR: could not decode answer from model: {'claim': \"The death of Rabah Saadane the former coach of the Algerian national team We belong to God and to Him we shall return Our deepest condolences to all brothers in our second homeland Algeria\",'statement': \"Arab Women's Cup draw (Egypt 2021): Algeria is in the second group along with the Jordanian and Palestine teams \",'SSE': \" Football Algeria team' Authority Description: 'Official account of football Algeria teams Lesverts 123vivalalger\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.98}\n",
      "ERROR: could not decode answer from model: ```json\n",
      "ERROR: could not decode answer from model: {\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", Confidence score: 0.85}\n",
      "ERROR: could not decode answer from model: { \"SUPPORTS\" :  \"naturalization decree in preparation: Lebanese passports for sale!\"}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.85}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {'claim': \"Bank of Lebanon: Trading volume on the Sayrafa platform reached $35 million today at a rate of 30 300 pounds to the dollar\", 'confidence': 0.8, 'decision': 'REFUTES'}\n",
      "ERROR: could not decode answer from model: \"NOT ENOUGH INFO\", Confidence: 1\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.7}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.9} \n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.6}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not decode answer from model: { REFUTES, 0.9 }\n",
      "ERROR: could not decode answer from model: {\"claim\": \"LIBYA - Sources of the event: News of the arrival of an American delegation to Benghazi to discuss efforts for a solution\", \"label\": \"SUPPORTS\", \"confidence\": \"80\"}\n",
      "ERROR: could not decode answer from model: ```\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 1}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 1}\n",
      "ERROR: could not decode answer from model: ```\n",
      "ERROR: could not decode answer from model: ```json\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", confidence: 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9} \n",
      "ERROR: could not decode answer from model: {\"REFUTES\", confidence: 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: ```json\n",
      "ERROR: could not decode answer from model: {\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: ```json\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not decode answer from model: {'REFUTES': 0.9}\n",
      "ERROR: could not decode answer from model: ```\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1.0}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.95}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\", 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.45}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 0.7}\n",
      "Error (429): 429; Text: {\"error\":\"Rate limit reached. You reached PRO hourly usage limit. Use Inference Endpoints (dedicated) to scale your endpoint.\"}; sleeping 1 hour...\n",
      "Error (4xx): 429; Text: {\"error\":\"Rate limit reached. You reached PRO hourly usage limit. Use Inference Endpoints (dedicated) to scale your endpoint.\"}; retrying... (retries=0)\n",
      "sleeping for 4 seconds before retrying (retries=1)\n",
      "ERROR: could not decode answer from model: {\"REFUTES\": 1}\n",
      "result for verification run - Macro-F1: 0.6291 Strict-Macro-F1: 0.6192 with retriever: rerank-nv-embed-v1 and retriever: llama3-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__transformers-roberta on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceee5a06473a4857959207533bfc0905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.2645 Strict-Macro-F1: 0.2645 with retriever: rerank-nv-embed-v1 and retriever: transformers-roberta\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__transformers-bart on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd60b9d1b7874a60bb6568a1d077a7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisk\\miniconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.2707 Strict-Macro-F1: 0.2707 with retriever: rerank-nv-embed-v1 and retriever: transformers-bart\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__openai-4o-mini on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de8ad88c54464e883d681d821018f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t178002\n",
      "prompt tokens:\t169793\n",
      "completion tokens:\t8209\n",
      "price estimate:\t$1.9442000000000002\n",
      "result for verification run - Macro-F1: 0.7818 Strict-Macro-F1: 0.7687 with retriever: rerank-nv-embed-v1 and retriever: openai-4o-mini\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__openai-4o on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb08e1b6d114a43b2e1b712548245bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t177373\n",
      "prompt tokens:\t167537\n",
      "completion tokens:\t9836\n",
      "price estimate:\t$1.97045\n",
      "result for verification run - Macro-F1: 0.8585 Strict-Macro-F1: 0.8505 with retriever: rerank-nv-embed-v1 and retriever: openai-4o\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Retrieval_Method</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.858523</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>openai-4o</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>1826.734934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.850244</td>\n",
       "      <td>0.842376</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>openai-4o</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.008027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.803897</td>\n",
       "      <td>0.795360</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>openai-4o-mini</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.010043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.781789</td>\n",
       "      <td>0.768742</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>openai-4o-mini</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>1830.555874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.754723</td>\n",
       "      <td>0.743435</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.007510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.670238</td>\n",
       "      <td>0.664647</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-8b</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.007634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.629077</td>\n",
       "      <td>0.619151</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>4338.367209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.628011</td>\n",
       "      <td>0.616134</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>llama3-8b</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>0.010030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.322321</td>\n",
       "      <td>0.322321</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.008018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283468</td>\n",
       "      <td>0.275085</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>transformers-bart</td>\n",
       "      <td>nopre-nonam-nobio</td>\n",
       "      <td>0.008036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.270719</td>\n",
       "      <td>0.270719</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>transformers-bart</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>200.267504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>rerank-nv-embed-v1</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nam-bio</td>\n",
       "      <td>115.391862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Macro-F1  Strict-Macro-F1    Retrieval_Method       Verifier_Method  \\\n",
       "11  0.858523         0.850470  rerank-nv-embed-v1             openai-4o   \n",
       "5   0.850244         0.842376  rerank-nv-embed-v1             openai-4o   \n",
       "4   0.803897         0.795360  rerank-nv-embed-v1        openai-4o-mini   \n",
       "10  0.781789         0.768742  rerank-nv-embed-v1        openai-4o-mini   \n",
       "1   0.754723         0.743435  rerank-nv-embed-v1            llama3-70b   \n",
       "0   0.670238         0.664647  rerank-nv-embed-v1             llama3-8b   \n",
       "7   0.629077         0.619151  rerank-nv-embed-v1            llama3-70b   \n",
       "6   0.628011         0.616134  rerank-nv-embed-v1             llama3-8b   \n",
       "2   0.322321         0.322321  rerank-nv-embed-v1  transformers-roberta   \n",
       "3   0.283468         0.275085  rerank-nv-embed-v1     transformers-bart   \n",
       "9   0.270719         0.270719  rerank-nv-embed-v1     transformers-bart   \n",
       "8   0.264516         0.264516  rerank-nv-embed-v1  transformers-roberta   \n",
       "\n",
       "          DS_Settings     Time (s)  \n",
       "11        pre-nam-bio  1826.734934  \n",
       "5   nopre-nonam-nobio     0.008027  \n",
       "4   nopre-nonam-nobio     0.010043  \n",
       "10        pre-nam-bio  1830.555874  \n",
       "1   nopre-nonam-nobio     0.007510  \n",
       "0   nopre-nonam-nobio     0.007634  \n",
       "7         pre-nam-bio  4338.367209  \n",
       "6         pre-nam-bio     0.010030  \n",
       "2   nopre-nonam-nobio     0.008018  \n",
       "3   nopre-nonam-nobio     0.008036  \n",
       "9         pre-nam-bio   200.267504  \n",
       "8         pre-nam-bio   115.391862  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "import pickle as pkl\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "\n",
    "for dataset_variation in selected_variations:\n",
    "\n",
    "    for exp_fingerprint in setups:\n",
    "        # get the dataset here since it is modified in place here, contrary to RQ2\n",
    "        dataset: AuredDataset = dataset_variations_dict[dataset_variation]\n",
    "        start = time.time()\n",
    "\n",
    "        # retrieved_data = retrieve_evidence(dataset, setups[exp_fingerprint]['retriever'])\n",
    "        data_path = f'{root_dir}/RQ1/experiment-{split}/results/{dataset_variation}_{setups[exp_fingerprint][\"retriever\"]}.pkl'\n",
    "        print(f'loaded retrieval results from {data_path}')\n",
    "        retrieved_data = pkl.load(open(data_path, 'rb'))\n",
    "\n",
    "        dataset.add_trec_list_judgements(retrieved_data)\n",
    "\n",
    "        run_filename = f'{out_dir}/{dataset_variation}_{exp_fingerprint}.pkl'\n",
    "\n",
    "        # check if the file already exists from a previous run\n",
    "        if os.path.exists(run_filename):\n",
    "            print(f'found {run_filename}, loading from file')\n",
    "            verification_results = pkl.load(open(run_filename, 'rb'))\n",
    "        else:\n",
    "            print(f'running {exp_fingerprint} on {dataset_variation}')\n",
    "            verification_results = run_verifier_on_dataset(\n",
    "                dataset=dataset,\n",
    "                verifier=setups[exp_fingerprint]['verifier'],\n",
    "                judge=solomon,\n",
    "                blind=False,\n",
    "            )\n",
    "            pkl.dump(verification_results, open(run_filename, 'wb'))\n",
    "\n",
    "        # print(verification_results)\n",
    "\n",
    "        macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "        retriever_label, verifier_label = exp_fingerprint.split('__')\n",
    "\n",
    "        print(\n",
    "            f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with retriever: {retriever_label} and retriever: {verifier_label}\"\n",
    "        )\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        data.append({\n",
    "            'Macro-F1': macro_f1,\n",
    "            'Strict-Macro-F1': strict_macro_f1,\n",
    "            'Retrieval_Method': retriever_label, \n",
    "            'Verifier_Method': verifier_label, \n",
    "            'DS_Settings': dataset_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
