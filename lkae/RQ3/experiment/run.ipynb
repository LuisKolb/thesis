{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.10 (built by craigm on 2024-08-22 17:33) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkls, root_dir, AuredDataset\n",
    "from lkae.retrieval.retrieve import get_retriever,retrieve_evidence\n",
    "from lkae.verification.verify import get_verifier, Judge, run_verifier_on_dataset\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "import pyterrier as pt\n",
    "import pyterrier.io as ptio\n",
    "import pyterrier.pipelines as ptpipelines\n",
    "from ir_measures import R, MAP    \n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ3\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"nopre-nonam-nobio\", \"pre-nam-bio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rerank-nv-embed-v1__llama3-8b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x208ce322170>},\n",
       " 'rerank-nv-embed-v1__llama3-70b': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x20907de7880>},\n",
       " 'rerank-nv-embed-v1__transformers-roberta': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x20907dcb2b0>},\n",
       " 'rerank-nv-embed-v1__transformers-bart': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x20910d76890>},\n",
       " 'rerank-nv-embed-v1__openai-4o-mini': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x2092951bb50>},\n",
       " 'rerank-nv-embed-v1__openai-4o': {'retriever': 'rerank-nv-embed-v1',\n",
       "  'verifier': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x2092a86df30>}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load each config and construct its retriever\n",
    "setups = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        exp_fingerprint = f'{config[\"retriever_method\"]}__{config[\"verifier_method\"]}'\n",
    "        \n",
    "        # retriever = get_retriever(**config)\n",
    "        verifier = get_verifier(**config)\n",
    "        \n",
    "        setups[exp_fingerprint] = {}\n",
    "        setups[exp_fingerprint]['retriever'] = config[\"retriever_method\"]\n",
    "        setups[exp_fingerprint]['verifier'] = verifier\n",
    "\n",
    "display(setups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-8b on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd435bf2922940869285eadf4ce4f744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (429): 429; Text: {\"error\":\"Rate limit reached. You reached PRO hourly usage limit. Use Inference Endpoints (dedicated) to scale your endpoint.\"}; sleeping 1 hour...\n",
      "Error (4xx): 429; Text: {\"error\":\"Rate limit reached. You reached PRO hourly usage limit. Use Inference Endpoints (dedicated) to scale your endpoint.\"}; retrying... (retries=0)\n",
      "sleeping for 4 seconds before retrying (retries=1)\n",
      "result for verification run - Macro-F1: 0.6702 Strict-Macro-F1: 0.6646 with retriever: rerank-nv-embed-v1 and retriever: llama3-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-70b on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfaf1104d8446bab8bfc9653035e664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unkown label \" SUPPORTS\" in answer: {\"decision\": \" SUPPORTS\", \"confidence\": 1}\n",
      "Error (429): 429; Text: {\"error\":\"Model is overloaded\",\"error_type\":\"overloaded\"}; sleeping 1 hour...\n",
      "Error (4xx): 429; Text: {\"error\":\"Model is overloaded\",\"error_type\":\"overloaded\"}; retrying... (retries=0)\n",
      "sleeping for 4 seconds before retrying (retries=1)\n",
      "Error (429): 429; Text: {\"error\":\"Model is overloaded\",\"error_type\":\"overloaded\"}; sleeping 1 hour...\n",
      "Error (4xx): 429; Text: {\"error\":\"Model is overloaded\",\"error_type\":\"overloaded\"}; retrying... (retries=0)\n",
      "sleeping for 4 seconds before retrying (retries=1)\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"NOT ENOUGH INFO\", \"confidence score\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": [\"REFUTES\"], \"confidence\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"SUPPORTS\", \"confidence_score\": 0.8}\n",
      "result for verification run - Macro-F1: 0.7547 Strict-Macro-F1: 0.7434 with retriever: rerank-nv-embed-v1 and retriever: llama3-70b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__transformers-roberta on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812330743881436fb2661fbe918240f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.3223 Strict-Macro-F1: 0.3223 with retriever: rerank-nv-embed-v1 and retriever: transformers-roberta\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__transformers-bart on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439235f1853b4c56b4b69f10a4223c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisk\\miniconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.2835 Strict-Macro-F1: 0.2751 with retriever: rerank-nv-embed-v1 and retriever: transformers-bart\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__openai-4o-mini on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b473edf3a4230b61d6a85f051802a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t178532\n",
      "prompt tokens:\t170298\n",
      "completion tokens:\t8234\n",
      "price estimate:\t$1.9500000000000002\n",
      "result for verification run - Macro-F1: 0.8039 Strict-Macro-F1: 0.7954 with retriever: rerank-nv-embed-v1 and retriever: openai-4o-mini\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/nopre-nonam-nobio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__openai-4o on nopre-nonam-nobio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca7ea9b024b424ba7c3762fec5fd6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t177794\n",
      "prompt tokens:\t168042\n",
      "completion tokens:\t9752\n",
      "price estimate:\t$1.97298\n",
      "result for verification run - Macro-F1: 0.8502 Strict-Macro-F1: 0.8424 with retriever: rerank-nv-embed-v1 and retriever: openai-4o\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-8b on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b9afb1d81d4498b14c19f921dd618c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.6280 Strict-Macro-F1: 0.6161 with retriever: rerank-nv-embed-v1 and retriever: llama3-8b\n",
      "loaded retrieval results from c:\\users\\luisk\\projects-win\\thesis\\lkae/RQ1/experiment-train_dev_combined/results/pre-nam-bio_rerank-nv-embed-v1.pkl\n",
      "running rerank-nv-embed-v1__llama3-70b on pre-nam-bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d4978dc7c24c74a31a98f1f701d2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES: 0.7\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", 0.85}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES, Confidence: 0.9\n",
      "\n",
      "The statement does not mention a Qatari being killed in Tunisia, nor does it mention the city of Bizerte. The statement actually talks about the humanitarian situation in Palestine, which suggests that it is unrelated to the claim.\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", 0.8}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES, 1\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.6}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES: 0.8\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES, 0.65\n",
      "ERROR: could not find the answer format in answer from model: SUPPORTS, Confidence: 0.9\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\", confidence: 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\", 0.98}\n",
      "ERROR: could not find the answer format in answer from model: {\"answer\": \"NOT ENOUGH INFO\", \"confidence\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"NOT ENOUGH INFO\", 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", confidence: 0.95}\n",
      "ERROR: could not find the answer format in answer from model: NOT ENOUGH INFO, 0.67\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\", confidence: 0.9}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES, Confidence: 0.7 \n",
      "The statement appears to be an article about the African Super Cup, specifically the match between Al-Ahly and Berkane, talking about the coach's statements. This Statement does not mention Samar Hosni or the reasoning about her being kept away; therefore it does not support the claim, however, it still does relate in that it talks about the African Super Cup just like the claim.\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", 0.8}\n",
      "ERROR: could not find the answer format in answer from model: SUPPORTS\n",
      "Confidence: 0.7\n",
      "ERROR: could not find the answer format in answer from model: REFUTES, Confidence: 0.8\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.7}\n",
      "ERROR: could not find the answer format in answer from model: {\"NOT ENOUGH INFO\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1} is not correct and we cant use new information.\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1.0}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES \n",
      "confidence: 0.9 \n",
      "\n",
      "Statement does not mention anything about Al-Ahly management putting pressure on CAF.\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": \"0.9\"}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"NOT ENOUGH INFO\", \"confidence_score\": 0.5}\n",
      "ERROR: could not find the answer format in answer from model: REFUTES, Confidence: 1.0\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"REFUTES\", \"confidence_score\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"claim\": REFUTES, \"confidence\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"claim\": \"By presidential order canceling the Corona test for Algerians at border crossings Algeria Border Crossings Tunisia Coronavirus\", \"statement\": \"Important: The President of the Republic Mr Abdelmadjid Tebboune honors the first and second best students nationally in the baccalaureate degree Ben Abbas Iman and Manar Merdas The honoring ceremony was attended by the Advisor to the President of the Republic in charge of Legal and Judicial Affairs the Acting Director of the Bureau Mr Boualem Bou\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0}\n",
      " Confidence score: 0\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.95}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 1.0}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", 0.5}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", confidence: 0.8}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0.8}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_fingerprint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_variation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     verification_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_verifier_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexp_fingerprint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mverifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjudge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolomon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     pkl\u001b[38;5;241m.\u001b[39mdump(verification_results, \u001b[38;5;28mopen\u001b[39m(run_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# print(verification_results)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\luisk\\projects-win\\thesis\\lkae\\verification\\verify.py:196\u001b[0m, in \u001b[0;36mrun_verifier_on_dataset\u001b[1;34m(dataset, verifier, judge, blind)\u001b[0m\n\u001b[0;32m    193\u001b[0m logger_text_score\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Verifying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrumor_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclaim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# print(f'({i+1}/{len(dataset)}) Verifying {rumor_id}: \"{claim}\"')\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m pred_label, pred_evidence \u001b[38;5;241m=\u001b[39m \u001b[43mjudge_using_evidence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrumor_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_evidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blind:\n\u001b[0;32m    199\u001b[0m     logger_text_score\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\luisk\\projects-win\\thesis\\lkae\\verification\\verify.py:149\u001b[0m, in \u001b[0;36mjudge_using_evidence\u001b[1;34m(rumor_id, claim, evidence, verifier, judge)\u001b[0m\n\u001b[0;32m    146\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevidence text empty for rumor with id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrumor_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; evidence=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclaim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m evidences_with_decisions\u001b[38;5;241m.\u001b[39mappend((claim,post,prediction))\n\u001b[0;32m    152\u001b[0m formatted_text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, post\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;66;03m# replace linebreaks, etc. for pretty printing in a single line\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\luisk\\projects-win\\thesis\\lkae\\verification\\types.py:17\u001b[0m, in \u001b[0;36mBaseVerifier.__call__\u001b[1;34m(self, claim, evidence, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, claim: \u001b[38;5;28mstr\u001b[39m, evidence: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VerificationResult:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify(claim, evidence, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\luisk\\projects-win\\thesis\\lkae\\verification\\models\\llama3_hf.py:91\u001b[0m, in \u001b[0;36mHFLlama3Verifier.verify\u001b[1;34m(self, claim, evidence)\u001b[0m\n\u001b[0;32m     89\u001b[0m confidence \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# remove quotes from confidence before converting, if present (for example: confidence='\"1\"')\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_labels:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerificationResult(label, confidence)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '0.8}'"
     ]
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "import pickle as pkl\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "\n",
    "for dataset_variation in selected_variations:\n",
    "\n",
    "    for exp_fingerprint in setups:\n",
    "        # get the dataset here since it is modified in place here, contrary to RQ2\n",
    "        dataset: AuredDataset = dataset_variations_dict[dataset_variation]\n",
    "        start = time.time()\n",
    "\n",
    "        # retrieved_data = retrieve_evidence(dataset, setups[exp_fingerprint]['retriever'])\n",
    "        data_path = f'{root_dir}/RQ1/experiment-{split}/results/{dataset_variation}_{setups[exp_fingerprint][\"retriever\"]}.pkl'\n",
    "        print(f'loaded retrieval results from {data_path}')\n",
    "        retrieved_data = pkl.load(open(data_path, 'rb'))\n",
    "\n",
    "        dataset.add_trec_list_judgements(retrieved_data)\n",
    "\n",
    "        run_filename = f'{out_dir}/{dataset_variation}_{exp_fingerprint}.pkl'\n",
    "\n",
    "        # check if the file already exists from a previous run\n",
    "        if os.path.exists(run_filename):\n",
    "            print(f'found {run_filename}, loading from file')\n",
    "            verification_results = pkl.load(open(run_filename, 'rb'))\n",
    "        else:\n",
    "            print(f'running {exp_fingerprint} on {dataset_variation}')\n",
    "            verification_results = run_verifier_on_dataset(\n",
    "                dataset=dataset,\n",
    "                verifier=setups[exp_fingerprint]['verifier'],\n",
    "                judge=solomon,\n",
    "                blind=False,\n",
    "            )\n",
    "            pkl.dump(verification_results, open(run_filename, 'wb'))\n",
    "\n",
    "        # print(verification_results)\n",
    "\n",
    "        macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "        retriever_label, verifier_label = exp_fingerprint.split('__')\n",
    "\n",
    "        print(\n",
    "            f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with retriever: {retriever_label} and retriever: {verifier_label}\"\n",
    "        )\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        data.append({\n",
    "            'Macro-F1': macro_f1,\n",
    "            'Strict-Macro-F1': strict_macro_f1,\n",
    "            'Retrieval_Method': retriever_label, \n",
    "            'Verifier_Method': verifier_label, \n",
    "            'DS_Settings': dataset_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
