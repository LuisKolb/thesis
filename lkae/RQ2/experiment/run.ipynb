{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, load_pkls, root_dir, AuredDataset\n",
    "from lkae.verification.verify import get_verifier\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "from lkae.verification.verify import Judge, run_verifier_on_dataset\n",
    "from lkae.utils.data_loading import AuthorityPost\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ2\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"pre-nonam-nobio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sbert-deberta-tasksource': <lkae.verification.models.deberta_verifier.DebertaVerifier at 0x1b6f3512590>,\n",
       " 'transformers-roberta': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x1b6ccbf7a60>,\n",
       " 'transformers-bart': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x1b6cd0c56c0>,\n",
       " 'llama3-1-8B': <lkae.verification.models.llama3_azure_ai.Llama3AzureVerifier at 0x1b6d1465780>,\n",
       " 'llama3-1-70b': <lkae.verification.models.llama3_azure_ai.Llama3AzureVerifier at 0x1b6d14657b0>,\n",
       " 'llama3-1-405b': <lkae.verification.models.llama3_azure_ai.Llama3AzureVerifier at 0x1b6d29b4850>,\n",
       " 'openai-4o-mini': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x1b6d29b54e0>,\n",
       " 'openai-4o': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x1b6d26f72b0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its verifier\n",
    "\n",
    "verifiers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        verifier_label = get_verifier(**config)\n",
    "        verifiers[config['verifier_method']] = verifier_label\n",
    "\n",
    "verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found results/pre-nonam-nobio_sbert-deberta-tasksource.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.4958 Strict-Macro-F1: 0.4958 with verifier sbert-deberta-tasksource and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_transformers-roberta.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7712 Strict-Macro-F1: 0.7712 with verifier transformers-roberta and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_transformers-bart.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7058 Strict-Macro-F1: 0.7058 with verifier transformers-bart and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7951ad9ee34258b9e6a34d355b906f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not json-parse response from Azure API: I cannot provide a response that contains harmful misinformation about vaccination. Can I help you with something else?, returning NOT ENOUGH INFO answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t91455\n",
      "prompt tokens:\t85930\n",
      "completion tokens:\t5525\n",
      "price estimate:\t$0.02914925\n",
      "result for verification run - Macro-F1: 0.8031 Strict-Macro-F1: 0.8031 with verifier llama3-1-8B and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609c0c83dd7148a4bc31e7b7b98aa843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t91441\n",
      "prompt tokens:\t85930\n",
      "completion tokens:\t5511\n",
      "price estimate:\t$0.24980134000000004\n",
      "result for verification run - Macro-F1: 0.9607 Strict-Macro-F1: 0.9607 with verifier llama3-1-70b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265c80b940394a49bb2617f02e7292d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not json-parse response from Azure API: {\"decision\": \"REFUTES\", \"confidence\": 0.8} \n",
      "\n",
      "The statement from the WHO authority account confirms the emergence of the first cases of the new Coronavirus in the UAE, but it mentions only 4 members of the same family being infected, which contradicts the claim of 75 cases in Abu Dhabi and 63 cases in Dubai. This discrepancy suggests that the claim is likely an exaggeration or false., returning NOT ENOUGH INFO answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----total token usage for verification-----\n",
      "total tokens:\t91492\n",
      "prompt tokens:\t85930\n",
      "completion tokens:\t5562\n",
      "price estimate:\t$0.4643282\n",
      "result for verification run - Macro-F1: 0.9853 Strict-Macro-F1: 0.9853 with verifier llama3-1-405b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_openai-4o-mini.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.9035 Strict-Macro-F1: 0.9035 with verifier openai-4o-mini and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_openai-4o.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.9377 Strict-Macro-F1: 0.9377 with verifier openai-4o and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.985261</td>\n",
       "      <td>0.985261</td>\n",
       "      <td>llama3-1-405b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>935.063797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.960686</td>\n",
       "      <td>0.960686</td>\n",
       "      <td>llama3-1-70b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>326.515228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.937672</td>\n",
       "      <td>0.937672</td>\n",
       "      <td>openai-4o</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.903537</td>\n",
       "      <td>0.903537</td>\n",
       "      <td>openai-4o-mini</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.011037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.803084</td>\n",
       "      <td>0.803084</td>\n",
       "      <td>llama3-1-8B</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>125.431390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771243</td>\n",
       "      <td>0.771243</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.010556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.705831</td>\n",
       "      <td>0.705831</td>\n",
       "      <td>transformers-bart</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.010717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.495774</td>\n",
       "      <td>0.495774</td>\n",
       "      <td>sbert-deberta-tasksource</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.013042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Macro-F1  Strict-Macro-F1           Verifier_Method      DS_Settings  \\\n",
       "5  0.985261         0.985261             llama3-1-405b  pre-nonam-nobio   \n",
       "4  0.960686         0.960686              llama3-1-70b  pre-nonam-nobio   \n",
       "7  0.937672         0.937672                 openai-4o  pre-nonam-nobio   \n",
       "6  0.903537         0.903537            openai-4o-mini  pre-nonam-nobio   \n",
       "3  0.803084         0.803084               llama3-1-8B  pre-nonam-nobio   \n",
       "1  0.771243         0.771243      transformers-roberta  pre-nonam-nobio   \n",
       "2  0.705831         0.705831         transformers-bart  pre-nonam-nobio   \n",
       "0  0.495774         0.495774  sbert-deberta-tasksource  pre-nonam-nobio   \n",
       "\n",
       "     Time (s)  \n",
       "5  935.063797  \n",
       "4  326.515228  \n",
       "7    0.011043  \n",
       "6    0.011037  \n",
       "3  125.431390  \n",
       "1    0.010556  \n",
       "2    0.010717  \n",
       "0    0.013042  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "import pickle as pkl\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for dataset_variation in selected_variations:\n",
    "    dataset: AuredDataset = dataset_variations_dict[dataset_variation]\n",
    "    for i, item in enumerate(dataset):\n",
    "        retrieved_ev = []\n",
    "        evidences = item[\"evidence\"]\n",
    "        if evidences is None:\n",
    "            print(f\"skipped {i} because no evidence\")\n",
    "            continue\n",
    "        for ev in evidences:\n",
    "            retrieved_ev.append(AuthorityPost(ev.url, ev.post_id, ev.text, 1, 1))\n",
    "        dataset[i][\"retrieved_evidence\"] = retrieved_ev\n",
    "        \n",
    "    for verifier_label in verifiers:\n",
    "        start = time.time()\n",
    "\n",
    "        run_filename = f'{out_dir}/{dataset_variation}_{verifier_label}.pkl'\n",
    "\n",
    "        # check if the file already exists from a previous run\n",
    "        if os.path.exists(run_filename):\n",
    "            print(f'found {run_filename}, loading from file')\n",
    "            verification_results = pkl.load(open(run_filename, 'rb'))\n",
    "        else:\n",
    "            verification_results = run_verifier_on_dataset(\n",
    "                dataset=dataset,\n",
    "                verifier=verifiers[verifier_label],\n",
    "                judge=solomon,\n",
    "                blind=False,\n",
    "            )\n",
    "            pkl.dump(verification_results, open(run_filename, 'wb'))\n",
    "\n",
    "        # print(verification_results)\n",
    "\n",
    "        macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "        print(\n",
    "            f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with verifier {verifier_label} and ground truth file {gold_file}\"\n",
    "        )\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        \n",
    "        data.append({\n",
    "            'Macro-F1': macro_f1,\n",
    "            'Strict-Macro-F1': strict_macro_f1,\n",
    "            'Verifier_Method': verifier_label, \n",
    "            'DS_Settings': dataset_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
