{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, root_dir, AuredDataset\n",
    "from lkae.verification.verify import get_verifier\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "from lkae.verification.verify import Judge, run_verifier_on_dataset\n",
    "from lkae.utils.data_loading import AuthorityPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# walk through the pkl directory and load all the datasets in one of its subdirectories\n",
    "# load each dataset with its subdirectory name and filename as the key\n",
    "# skip non-pkl files\n",
    "for subdir in os.listdir(pkl_dir):\n",
    "    if not os.path.isdir(os.path.join(pkl_dir, subdir)):\n",
    "        continue            \n",
    "    datasets[subdir] = {}\n",
    "    for filename in os.listdir(os.path.join(pkl_dir, subdir)):\n",
    "        if not filename.endswith('.pkl'):\n",
    "            continue\n",
    "        key = os.path.join(subdir, filename)\n",
    "        datasets[subdir][filename.split('.')[0]] = load_pkl(os.path.join(pkl_dir, key))\n",
    "\n",
    "split = 'dev'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "# ground truth RQ2\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a single variation of the dataset\n",
    "selected_variation = \"pre-nonam-nobio\"\n",
    "dataset: AuredDataset = dataset_variations_dict[selected_variation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transformers-roberta': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x1f76d4d2b00>,\n",
       " 'transformers-bart': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x1f7a8dc2530>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its verifier\n",
    "\n",
    "verifiers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        verifier_label = get_verifier(**config)\n",
    "        verifiers[config['verifier_method']] = verifier_label\n",
    "\n",
    "verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    retrieved_ev = []\n",
    "    evidences = item[\"evidence\"]\n",
    "    if evidences is None:\n",
    "        print(f\"skipped {i} because no evidence\")\n",
    "        continue\n",
    "    for ev in evidences:\n",
    "        retrieved_ev.append(AuthorityPost(ev.url, ev.post_id, ev.text, 1, 1))\n",
    "    dataset[i][\"retrieved_evidence\"] = retrieved_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_045\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_025\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_026\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_088\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_066\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_053\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_046\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_059\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_033\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_001\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_039\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_076\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.6993 Strict-Macro-F1: 0.6993 with config {'verifier_method': 'transformers-bart', 'model': 'facebook/bart-large-mnli'} and TREC FILE c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisk\\miniconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_045\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_025\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_026\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_088\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_066\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_053\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_046\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_059\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_033\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_001\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_039\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_076\n",
      "key \"retrieved_evidence\" was empty for rumor with id AuRED_003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.6897 Strict-Macro-F1: 0.6897 with config {'verifier_method': 'transformers-bart', 'model': 'facebook/bart-large-mnli'} and TREC FILE c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.699346</td>\n",
       "      <td>0.699346</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>4.848028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.689724</td>\n",
       "      <td>0.689724</td>\n",
       "      <td>transformers-bart</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>6.199241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Macro-F1  Strict-Macro-F1       Verifier_Method      DS_Settings  Time (s)\n",
       "0  0.699346         0.699346  transformers-roberta  pre-nonam-nobio  4.848028\n",
       "1  0.689724         0.689724     transformers-bart  pre-nonam-nobio  6.199241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for verifier_label in verifiers:\n",
    "    start = time.time()\n",
    "\n",
    "    verification_results = run_verifier_on_dataset(\n",
    "        dataset=dataset,\n",
    "        verifier=verifiers[verifier_label],\n",
    "        judge=solomon,\n",
    "        blind=False,\n",
    "    )\n",
    "\n",
    "    # print(verification_results)\n",
    "\n",
    "    macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "    print(\n",
    "        f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with config {config} and TREC FILE {gold_file}\"\n",
    "    )\n",
    "\n",
    "    wall_time = time.time() - start\n",
    "\n",
    "    \n",
    "    data.append({\n",
    "        'Macro-F1': macro_f1,\n",
    "        'Strict-Macro-F1': strict_macro_f1,\n",
    "        'Verifier_Method': verifier_label, \n",
    "        'DS_Settings': selected_variation,\n",
    "        'Time (s)': wall_time,\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
