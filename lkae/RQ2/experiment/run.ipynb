{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compare multiple runs of a LLM verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, root_dir, AuredDataset\n",
    "from lkae.verification.verify import get_verifier\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "from lkae.verification.verify import Judge, run_verifier_on_dataset\n",
    "from lkae.utils.data_loading import AuthorityPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "# walk through the pkl directory and load all the datasets in one of its subdirectories\n",
    "# load each dataset with its subdirectory name and filename as the key\n",
    "# skip non-pkl files\n",
    "for subdir in os.listdir(pkl_dir):\n",
    "    if not os.path.isdir(os.path.join(pkl_dir, subdir)):\n",
    "        continue            \n",
    "    datasets[subdir] = {}\n",
    "    for filename in os.listdir(os.path.join(pkl_dir, subdir)):\n",
    "        if not filename.endswith('.pkl'):\n",
    "            continue\n",
    "        key = os.path.join(subdir, filename)\n",
    "        datasets[subdir][filename.split('.')[0]] = load_pkl(os.path.join(pkl_dir, key))\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'dev'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())\n",
    "\n",
    "# ground truth RQ2\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"pre-nonam-nobio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transformers-roberta': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x1e46c611510>,\n",
       " 'transformers-bart': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x1e4a3fd1ba0>,\n",
       " 'llama3-8b': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x1e4a527ee30>,\n",
       " 'llama3-70b': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x1e4a57ec700>,\n",
       " 'llama3-405b': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x1e4a57dc700>,\n",
       " 'openai-4o-mini': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x1e4a5804700>,\n",
       " 'openai-4o': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x1e5025acdf0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its verifier\n",
    "\n",
    "verifiers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        verifier_label = get_verifier(**config)\n",
    "        verifiers[config['verifier_method']] = verifier_label\n",
    "\n",
    "verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.6889 Strict-Macro-F1: 0.6889 with verifier transformers-roberta and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisk\\miniconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for verification run - Macro-F1: 0.6787 Strict-Macro-F1: 0.6787 with verifier transformers-bart and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "result for verification run - Macro-F1: 0.8640 Strict-Macro-F1: 0.8640 with verifier llama3-8b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "result for verification run - Macro-F1: 1.0000 Strict-Macro-F1: 1.0000 with verifier llama3-70b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "result for verification run - Macro-F1: 0.9722 Strict-Macro-F1: 0.9722 with verifier llama3-405b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "-----total token usage for verification-----\n",
      "total tokens:\t16264\n",
      "prompt tokens:\t15502\n",
      "completion tokens:\t762\n",
      "price estimate:\t$0.17788\n",
      "result for verification run - Macro-F1: 1.0000 Strict-Macro-F1: 1.0000 with verifier openai-4o-mini and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "-----total token usage for verification-----\n",
      "total tokens:\t16203\n",
      "prompt tokens:\t15274\n",
      "completion tokens:\t929\n",
      "price estimate:\t$0.18061\n",
      "result for verification run - Macro-F1: 0.9722 Strict-Macro-F1: 0.9722 with verifier openai-4o and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_dev.jsonl\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>110.913317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>openai-4o-mini</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>183.767020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.972174</td>\n",
       "      <td>0.972174</td>\n",
       "      <td>llama3-405b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>335.902030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.972174</td>\n",
       "      <td>0.972174</td>\n",
       "      <td>openai-4o</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>180.976645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.863997</td>\n",
       "      <td>0.863997</td>\n",
       "      <td>llama3-8b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>22.043511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>4.249372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.678664</td>\n",
       "      <td>0.678664</td>\n",
       "      <td>transformers-bart</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>6.098403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Macro-F1  Strict-Macro-F1       Verifier_Method      DS_Settings  \\\n",
       "3  1.000000         1.000000            llama3-70b  pre-nonam-nobio   \n",
       "5  1.000000         1.000000        openai-4o-mini  pre-nonam-nobio   \n",
       "4  0.972174         0.972174           llama3-405b  pre-nonam-nobio   \n",
       "6  0.972174         0.972174             openai-4o  pre-nonam-nobio   \n",
       "2  0.863997         0.863997             llama3-8b  pre-nonam-nobio   \n",
       "0  0.688889         0.688889  transformers-roberta  pre-nonam-nobio   \n",
       "1  0.678664         0.678664     transformers-bart  pre-nonam-nobio   \n",
       "\n",
       "     Time (s)  \n",
       "3  110.913317  \n",
       "5  183.767020  \n",
       "4  335.902030  \n",
       "6  180.976645  \n",
       "2   22.043511  \n",
       "0    4.249372  \n",
       "1    6.098403  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for dataset_variation in selected_variations:\n",
    "    dataset: AuredDataset = dataset_variations_dict[dataset_variation]\n",
    "    for i, item in enumerate(dataset):\n",
    "        retrieved_ev = []\n",
    "        evidences = item[\"evidence\"]\n",
    "        if evidences is None:\n",
    "            print(f\"skipped {i} because no evidence\")\n",
    "            continue\n",
    "        for ev in evidences:\n",
    "            retrieved_ev.append(AuthorityPost(ev.url, ev.post_id, ev.text, 1, 1))\n",
    "        dataset[i][\"retrieved_evidence\"] = retrieved_ev\n",
    "        \n",
    "    for verifier_label in verifiers:\n",
    "        start = time.time()\n",
    "\n",
    "        verification_results = run_verifier_on_dataset(\n",
    "            dataset=dataset,\n",
    "            verifier=verifiers[verifier_label],\n",
    "            judge=solomon,\n",
    "            blind=False,\n",
    "        )\n",
    "\n",
    "        # print(verification_results)\n",
    "\n",
    "        macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "        print(\n",
    "            f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with verifier {verifier_label} and ground truth file {gold_file}\"\n",
    "        )\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        \n",
    "        data.append({\n",
    "            'Macro-F1': macro_f1,\n",
    "            'Strict-Macro-F1': strict_macro_f1,\n",
    "            'Verifier_Method': verifier_label, \n",
    "            'DS_Settings': dataset_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
