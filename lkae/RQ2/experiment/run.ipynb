{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nopre-nam-bio', 'nopre-nam-nobio', 'nopre-nonam-bio', 'nopre-nonam-nobio', 'pre-nam-bio', 'pre-nam-nobio', 'pre-nonam-bio', 'pre-nonam-nobio'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, load_pkls, root_dir, AuredDataset\n",
    "from lkae.verification.verify import get_verifier\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "from lkae.verification.verify import Judge, run_verifier_on_dataset\n",
    "from lkae.utils.data_loading import AuthorityPost\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ2\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"pre-nonam-nobio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sbert-deberta-tasksource': <lkae.verification.models.deberta_verifier.DebertaVerifier at 0x25e095111b0>,\n",
       " 'transformers-roberta': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x25e62d67a00>,\n",
       " 'transformers-bart': <lkae.verification.models.transformers_verifier.TransformersVerifier at 0x25e63231300>,\n",
       " 'llama3-8b': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x25e633c5cf0>,\n",
       " 'llama3-70b': <lkae.verification.models.llama3_hf.HFLlama3Verifier at 0x25eea038700>,\n",
       " 'openai-4o-mini': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x25eea06b880>,\n",
       " 'openai-4o': <lkae.verification.models.openai_verifier.OpenaiVerifier at 0x25e955e72e0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each config and construct its verifier\n",
    "\n",
    "verifiers = {}\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        verifier_label = get_verifier(**config)\n",
    "        verifiers[config['verifier_method']] = verifier_label\n",
    "\n",
    "verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True, # ignore NEI predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found results/pre-nonam-nobio_sbert-deberta-tasksource.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.4958 Strict-Macro-F1: 0.4958 with verifier sbert-deberta-tasksource and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_transformers-roberta.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7712 Strict-Macro-F1: 0.7712 with verifier transformers-roberta and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_transformers-bart.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7058 Strict-Macro-F1: 0.7058 with verifier transformers-bart and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "found results/pre-nonam-nobio_llama3-8b.pkl, loading from file\n",
      "result for verification run - Macro-F1: 0.7595 Strict-Macro-F1: 0.7595 with verifier llama3-8b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\", 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"SUPPORTS\", \"confidence_score\": 0.7}\n",
      "ERROR: could not find the answer format in answer from model: {\"claim\": \"SUPPORTS\", \"confidence\": \"0.8\"}\n",
      "ERROR: could not find the answer format in answer from model: {\"claim\": \"REFUTES\", \"confidence\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"REJECT REASONING\": \n",
      "\"Not related claims. Confidence score: 1.0\", \n",
      "\n",
      "\"reason\": \n",
      "\"The claim mentions a storming of a headquarters by a movement, the statement does not mention it, \n",
      "it talks about an application launch.\"},\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\", \"confidence\": 0.7}\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"CLAIMS\": \"NOT ENOUGH INFO\", \"confidence\": 0.55}\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"SUPPORTS\", \"confidence_score\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"answer\": \"SUPPORTS\", \"confidence_score\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"SUPPORTS\": true, \"confidence\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.7}\n",
      "ERROR: could not find the answer format in answer from model: {\"claim\": \"REFUTES\", \"confidence\": \"0.8\"}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"NOT ENOUGH INFO\", \"confidence score\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"NOT ENOUGH INFO\", \"confidence\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"NOT ENOUGH INFO\", \"confidence score\": 0.99}\n",
      "ERROR: could not find the answer format in answer from model: {\n",
      "  \"decision\": \"NOT ENOUGH INFO\", \n",
      "  \"confidence_score\": 0.8\n",
      "}\n",
      "ERROR: could not find the answer format in answer from model: {\"ANSWER\": \"SUPPORTS\", \"CONFIDENCE SCORE\": 1}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"SUPPORTS\", \"confidence_score\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"label\": \"REFUTES\", \"confidence\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"label\": \"REFUTES\", \"confidence\": 0.9}\n",
      "ERROR: could not find the answer format in answer from model: {\"REFUTES\": 0.7}\n",
      "ERROR: could not find the answer format in answer from model: { \"answer\": \"REFUTES\", \"confidence\": 0.8 }\n",
      "ERROR: could not find the answer format in answer from model: {\"answer\": \"REFUTES\", \"confidence\": 0.8}\n",
      "ERROR: could not find the answer format in answer from model: {\n",
      "  \"decision\": \"REFUTES\",\n",
      "  \"confidence_score\": 0.7\n",
      "}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"NOT ENOUGH INFO\", \"confidence_score\": 0.5}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"NOT ENOUGH INFO\", \"confidence score\": 0.5}\n",
      "ERROR: could not find the answer format in answer from model: {\"decision\": \"SUPPORTS\", \"confidence_score\": 1.0}\n",
      "result for verification run - Macro-F1: 0.8712 Strict-Macro-F1: 0.8712 with verifier llama3-70b and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "-----total token usage for verification-----\n",
      "total tokens:\t95523\n",
      "prompt tokens:\t90859\n",
      "completion tokens:\t4664\n",
      "price estimate:\t$1.04851\n",
      "result for verification run - Macro-F1: 0.9035 Strict-Macro-F1: 0.9035 with verifier openai-4o-mini and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "-----total token usage for verification-----\n",
      "total tokens:\t95095\n",
      "prompt tokens:\t89539\n",
      "completion tokens:\t5556\n",
      "price estimate:\t$1.06207\n",
      "result for verification run - Macro-F1: 0.9377 Strict-Macro-F1: 0.9377 with verifier openai-4o and ground truth file c:\\users\\luisk\\projects-win\\thesis\\lkae\\data\\English_train_dev_combined.jsonl\n",
      "saved df to results/df_verification.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Strict-Macro-F1</th>\n",
       "      <th>Verifier_Method</th>\n",
       "      <th>DS_Settings</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.937672</td>\n",
       "      <td>0.937672</td>\n",
       "      <td>openai-4o</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>1056.154317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.903537</td>\n",
       "      <td>0.903537</td>\n",
       "      <td>openai-4o-mini</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>1021.852168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.871184</td>\n",
       "      <td>0.871184</td>\n",
       "      <td>llama3-70b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>356.248933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771243</td>\n",
       "      <td>0.771243</td>\n",
       "      <td>transformers-roberta</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.008006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.759496</td>\n",
       "      <td>0.759496</td>\n",
       "      <td>llama3-8b</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.008519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.705831</td>\n",
       "      <td>0.705831</td>\n",
       "      <td>transformers-bart</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.010517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.495774</td>\n",
       "      <td>0.495774</td>\n",
       "      <td>sbert-deberta-tasksource</td>\n",
       "      <td>pre-nonam-nobio</td>\n",
       "      <td>0.009515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Macro-F1  Strict-Macro-F1           Verifier_Method      DS_Settings  \\\n",
       "6  0.937672         0.937672                 openai-4o  pre-nonam-nobio   \n",
       "5  0.903537         0.903537            openai-4o-mini  pre-nonam-nobio   \n",
       "4  0.871184         0.871184                llama3-70b  pre-nonam-nobio   \n",
       "1  0.771243         0.771243      transformers-roberta  pre-nonam-nobio   \n",
       "3  0.759496         0.759496                 llama3-8b  pre-nonam-nobio   \n",
       "2  0.705831         0.705831         transformers-bart  pre-nonam-nobio   \n",
       "0  0.495774         0.495774  sbert-deberta-tasksource  pre-nonam-nobio   \n",
       "\n",
       "      Time (s)  \n",
       "6  1056.154317  \n",
       "5  1021.852168  \n",
       "4   356.248933  \n",
       "1     0.008006  \n",
       "3     0.008519  \n",
       "2     0.010517  \n",
       "0     0.009515  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then for every variation of the dataset in ds, run the experiment with each retriever and save the results\n",
    "import pickle as pkl\n",
    "\n",
    "out_dir = 'results'\n",
    "data = []\n",
    "\n",
    "for dataset_variation in selected_variations:\n",
    "    dataset: AuredDataset = dataset_variations_dict[dataset_variation]\n",
    "    for i, item in enumerate(dataset):\n",
    "        retrieved_ev = []\n",
    "        evidences = item[\"evidence\"]\n",
    "        if evidences is None:\n",
    "            print(f\"skipped {i} because no evidence\")\n",
    "            continue\n",
    "        for ev in evidences:\n",
    "            retrieved_ev.append(AuthorityPost(ev.url, ev.post_id, ev.text, 1, 1))\n",
    "        dataset[i][\"retrieved_evidence\"] = retrieved_ev\n",
    "        \n",
    "    for verifier_label in verifiers:\n",
    "        start = time.time()\n",
    "\n",
    "        run_filename = f'{out_dir}/{dataset_variation}_{verifier_label}.pkl'\n",
    "\n",
    "        # check if the file already exists from a previous run\n",
    "        if os.path.exists(run_filename):\n",
    "            print(f'found {run_filename}, loading from file')\n",
    "            verification_results = pkl.load(open(run_filename, 'rb'))\n",
    "        else:\n",
    "            verification_results = run_verifier_on_dataset(\n",
    "                dataset=dataset,\n",
    "                verifier=verifiers[verifier_label],\n",
    "                judge=solomon,\n",
    "                blind=False,\n",
    "            )\n",
    "            pkl.dump(verification_results, open(run_filename, 'wb'))\n",
    "\n",
    "        # print(verification_results)\n",
    "\n",
    "        macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "        print(\n",
    "            f\"result for verification run - Macro-F1: {macro_f1:.4f} Strict-Macro-F1: {strict_macro_f1:.4f} with verifier {verifier_label} and ground truth file {gold_file}\"\n",
    "        )\n",
    "\n",
    "        wall_time = time.time() - start\n",
    "\n",
    "        \n",
    "        data.append({\n",
    "            'Macro-F1': macro_f1,\n",
    "            'Strict-Macro-F1': strict_macro_f1,\n",
    "            'Verifier_Method': verifier_label, \n",
    "            'DS_Settings': dataset_variation,\n",
    "            'Time (s)': wall_time,\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_verification = pd.DataFrame(data)\n",
    "\n",
    "df_verification.to_csv(f'{out_dir}/df_verification.csv')\n",
    "print(f'saved df to {out_dir}/df_verification.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_verification.sort_values(by='Macro-F1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
