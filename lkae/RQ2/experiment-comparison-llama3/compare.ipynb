{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lkae.utils.data_loading import pkl_dir, load_pkl, load_pkls, root_dir, AuredDataset\n",
    "from lkae.verification.verify import get_verifier\n",
    "from lkae.utils.scoring import eval_run_custom_nofile\n",
    "from lkae.verification.verify import Judge, run_verifier_on_dataset\n",
    "from lkae.utils.data_loading import AuthorityPost\n",
    "\n",
    "datasets = load_pkls(pkl_dir)\n",
    "\n",
    "# possilbe splits: train, dev, train_dev_combined\n",
    "# (test, all_combined don't have \"labels\")\n",
    "split = 'train_dev_combined'\n",
    "\n",
    "dataset_split = f'English_{split}'\n",
    "qrel_filename = f'{dataset_split}_qrels.txt'\n",
    "\n",
    "dataset_variations_dict = datasets[dataset_split]\n",
    "print(dataset_variations_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth RQ2\n",
    "gold_file = os.path.join(root_dir, 'data', f'{dataset_split}.jsonl')\n",
    "gold_list = [line for line in jsonlines.open(gold_file)]\n",
    "\n",
    "# select a set of variations of the dataset\n",
    "selected_variations = [\"pre-nonam-nobio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each config and construct its verifier\n",
    "\n",
    "verifiers = {}\n",
    "\n",
    "with open('config-compare.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "    for config in configs['configs']:\n",
    "        verifier_label = get_verifier(**config)\n",
    "        verifiers[config['verifier_method']] = verifier_label\n",
    "\n",
    "verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare outputs for different verifiers on one dataset\n",
    "# first get judgements for all verifiers\n",
    "\n",
    "# get the dataset\n",
    "selected_variation = selected_variations[0]\n",
    "dataset: AuredDataset = dataset_variations_dict[selected_variation]\n",
    "dataset = dataset[:]\n",
    "for i, item in enumerate(dataset):\n",
    "    retrieved_ev = []\n",
    "    evidences = item[\"evidence\"]\n",
    "    if evidences is None:\n",
    "        print(f\"skipped {i} because no evidence\")\n",
    "        continue\n",
    "    for ev in evidences:\n",
    "        retrieved_ev.append(AuthorityPost(ev.url, ev.post_id, ev.text, 1, 1))\n",
    "    dataset[i][\"retrieved_evidence\"] = retrieved_ev\n",
    "\n",
    "solomon = Judge(\n",
    "    scale=False,  # ignore scaling, weigh each evidence evenly, except for confidence score given by verifier\n",
    "    ignore_nei=True,  # ignore NEI predictions\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for verifier_label in verifiers:\n",
    "    start = time.time()\n",
    "\n",
    "    verification_results = run_verifier_on_dataset(\n",
    "        dataset=dataset,\n",
    "        verifier=verifiers[verifier_label],\n",
    "        judge=solomon,\n",
    "        blind=False,\n",
    "    )\n",
    "\n",
    "    macro_f1, strict_macro_f1 = eval_run_custom_nofile(verification_results, gold_list)\n",
    "\n",
    "    results[verifier_label] = {\n",
    "        \"res_dict\": verification_results,\n",
    "        \"time\": time.time() - start,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"strict_f1\": strict_macro_f1,\n",
    "        \"settings\": {\"verifier\": verifier_label, \"dataset\": selected_variation},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(results, open('results/results.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"transformers-roberta\": {\n",
    "#     \"res_dict\": [\n",
    "#       {\n",
    "#         \"id\": \"AuRED_142\",\n",
    "#         \"label\": \"REFUTES\",\n",
    "#         \"claim\": \"Naturalization decree in preparation: Lebanese passports for sale !\",\n",
    "#         \"predicted_label\": \"REFUTES\",\n",
    "#         \"predicted_evidence\": [\n",
    "#           [\n",
    "#             \"https://twitter.com/LBpresidency\",\n",
    "#             \"1555986659279360001\",\n",
    "#             \"Statement from Authority Account 'LBpresidency': ''The Information Office of the Presidency of the Republic denies a false news broadcast by the MTV station about Baabda Palace preparing a decree naturalizing 4 000 people and recalls that it had denied yesterday the false information published by the French magazine 'Liberation' about the same fabricated news ''\",\n",
    "#             0.5575303435325623\n",
    "#           ],\n",
    "#           [\n",
    "#             \"https://twitter.com/LBpresidency\",\n",
    "#             \"1555424541509386240\",\n",
    "#             \"Statement from Authority Account 'LBpresidency': ''The Information Office of the Presidency of the Republic: What was published by the French newspaper 'Liberation' about the 'selling' of Lebanese passports to non-Lebanese is false and baseless news '\",\n",
    "#             0.9313378930091858\n",
    "#           ]\n",
    "#         ]\n",
    "#       },\n",
    "#     ],\n",
    "#     \"time\": 1.071157693862915,\n",
    "#     \"macro_f1\": 0.27692307692307694,\n",
    "#     \"strict_f1\": 0.27692307692307694,\n",
    "#     \"settings\": {\n",
    "#       \"verifier\": \"transformers-roberta\",\n",
    "#       \"dataset\": \"pre-nonam-nobio\"\n",
    "#     }\n",
    "#   },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the comparison data\n",
    "comparison_data = {}\n",
    "\n",
    "# Determine the maximum number of evidence pieces across all results\n",
    "max_evidence = max(\n",
    "    max(len(item['predicted_evidence']) for item in result['res_dict'])\n",
    "    for result in results.values()\n",
    ")\n",
    "\n",
    "# clamp to max 5 evidence pieces\n",
    "max_evidence = min(max_evidence, 5)\n",
    "\n",
    "# Iterate through each result in the results dictionary\n",
    "for verifier_label, result in results.items():\n",
    "    for item in result['res_dict']:\n",
    "        item_id = item['id']\n",
    "        if item_id not in comparison_data:\n",
    "            comparison_data[item_id] = {\n",
    "                'id': item['id'],\n",
    "                'claim': item['claim'],\n",
    "                'label': item['label'],\n",
    "            }\n",
    "        \n",
    "        comparison_data[item_id][f'{verifier_label}-pred_label'] = item['predicted_label']\n",
    "        \n",
    "        # Add evidence columns with the new format\n",
    "        for i in range(max_evidence):\n",
    "            if i < len(item['predicted_evidence']):\n",
    "                ev = item['predicted_evidence'][i]\n",
    "                comparison_data[item_id][f'{verifier_label}-ev_{i+1}'] = f\"({ev[3]:.1f}) {ev[2]} \"\n",
    "            else:\n",
    "                comparison_data[item_id][f'{verifier_label}-ev_{i+1}'] = ''\n",
    "\n",
    "# Create a DataFrame from the comparison data\n",
    "df_comparison = pd.DataFrame(list(comparison_data.values()))\n",
    "\n",
    "# Define the column order\n",
    "columns = ['id', 'claim', 'label']\n",
    "for verifier in results.keys():\n",
    "    columns.append(f'{verifier}-pred_label')\n",
    "\n",
    "for verifier in results.keys():\n",
    "    for i in range(max_evidence):\n",
    "        columns.append(f'{verifier}-ev_{i+1}')\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_comparison = df_comparison[columns]\n",
    "\n",
    "# Add a column to indicate if predicted labels match\n",
    "def labels_match(row):\n",
    "    predicted_labels = [row[f'{verifier}-pred_label'] for verifier in results.keys()]\n",
    "    return 'Yes' if len(set(predicted_labels)) == 1 else 'No'\n",
    "\n",
    "df_comparison['Predicted Labels Match'] = df_comparison.apply(labels_match, axis=1)\n",
    "\n",
    "# Move the 'Predicted Labels Match' column to the end\n",
    "cols = df_comparison.columns.tolist()\n",
    "cols = cols[:-1] + cols[-1:]\n",
    "df_comparison = df_comparison[cols]\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_comparison)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file for easier viewing in spreadsheet software\n",
    "df_comparison.to_csv('results/verifier_comparison_detailed.csv', index=False)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for verifier_label, result in results.items():\n",
    "    print(f\"\\n{verifier_label}:\")\n",
    "    print(f\"  Macro F1: {result['macro_f1']:.4f}\")\n",
    "    print(f\"  Strict F1: {result['strict_f1']:.4f}\")\n",
    "    print(f\"  Time: {result['time']:.2f} seconds\")\n",
    "\n",
    "# Print the percentage of matching predictions\n",
    "matching_percentage = (df_comparison['Predicted Labels Match'] == 'Yes').mean() * 100\n",
    "print(f\"\\nPercentage of matching predictions: {matching_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
